
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://Alton1998.github.io/alton.github.io/blog/category/artificial-intelligence/">
      
      
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Artificial Intelligence - Alton Lavin D'Souza</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
    <meta name="google-site-verification" content="oXpHIsgH9DHTXwNuaKltPOEu7XdN7F5dS9DmfEjCWNo" />

  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#artificial-intelligence" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Alton Lavin D&#39;Souza" class="md-header__button md-logo" aria-label="Alton Lavin D'Souza" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Alton Lavin D'Souza
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Artificial Intelligence
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Alton Lavin D&#39;Souza" class="md-nav__button md-logo" aria-label="Alton Lavin D'Souza" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Alton Lavin D'Souza
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Work Experience
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Work Experience
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../work_experience/oracle/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Oracle
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Projects
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Projects
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../projects/arrhythmia/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Arrhythmia Detection
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../projects/chest_x_rays/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Corona Virus Detection in Chest X-Rays
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../projects/tpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensor Processing Unit
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Blog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content" data-md-component="content">
    <div class="md-content__inner">
      <header class="md-typeset">
        <h1 id="artificial-intelligence">Artificial Intelligence</h1>
      </header>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://avatars.githubusercontent.com/u/39479720?s=400&u=17af60674de436a0ef97d3e528947194708b03bd&v=4" alt="Alton Lavin D'souza">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2025-01-11 00:00:00+00:00">January 11, 2025</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../artificial-neural-networks/" class="md-meta__link">Artificial Neural Networks</a>, 
              <a href="./" class="md-meta__link">Artificial Intelligence</a>, 
              <a href="../llms/" class="md-meta__link">LLMs</a>, 
              <a href="../attention/" class="md-meta__link">Attention</a>, 
              <a href="../transformers/" class="md-meta__link">Transformers</a></li>
        
        
          
          <li class="md-meta__item">
            
              3 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="all-about-attention"><a class="toclink" href="../../2025/01/11/all-about-attention/">All about attention</a></h2>
<h3 id="overview"><a class="toclink" href="../../2025/01/11/all-about-attention/#overview">Overview</a></h3>
<p>Attention layers are now used over RNNs and even CNNs to speed up processing. In this blog we will see how attention layers are implemented.</p>
<h3 id="working-of-attention-layers"><a class="toclink" href="../../2025/01/11/all-about-attention/#working-of-attention-layers">Working of Attention layers</a></h3>
<p>There three inputs to attention layers:</p>
<ol>
<li>Query: Represents the "question" or "search term" for determining which parts of the input sequence are relevant.</li>
<li>Key: Represents the "descriptor" of each input element, used to decide how relevant each input element is to the query.</li>
<li>Values: Contains the actual information or representation of each input element that will be passed along after attention is computed.</li>
</ol>
<p>Given a Query and Key we calculate the similarity, this allows us to use the key with the max similarity and use its value for attention.</p>
<div class="arithmatex">\[
     \text{Score}(Q, K) = Q \cdot K^\top
\]</div>
<p>The above equation results in matrix describing how much importance a query gives to a key. In the equation <code>Q</code> is the query and <code>K</code> is the key.</p>
<p>The next step is scaling, we perform scaling to avoid large values, larger values require more resources for computation, So now the equation takes the following shape:</p>
<div class="arithmatex">\[
     \text{Scaled Score}(Q, K) = \frac{Q \cdot K^\top}{\sqrt{d_k}}
\]</div>
<p>Where <span class="arithmatex">\(d_k\)</span> is the dimensionality of the Key vectors.</p>
<p>The scores are passed through a softmax function to convert them into probabilities (attention weights). These probabilities determine the contribution of each input element. The equation now takes the following form:</p>
<div class="arithmatex">\[
     \text{Attention Weights} = \text{softmax}\left(\frac{Q \cdot K^\top}{\sqrt{d_k}}\right)
\]</div>
<p>Overall the equation would look something like this:</p>
<div class="arithmatex">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]</div>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="QKV" src="../../pics/QKV.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 1: Query Key Value</em></td>
</tr>
</tbody>
</table>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Scaled Dot PRoduct Attention" src="../../pics/Scaled%20Dot%20Product%20Attention.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 2: Flow of calculating Attention in Scaled Dot Product Attention</em></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Query Key maping" src="../../pics/Query_key_mapping.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 2: Example mapping similar query-key value pairs</em></td>
</tr>
</tbody>
</table>
<p></center></p>
<p>Lets try to understand this with an analogy. Consider the example where you are visiting a library and ask for a book. You say "I want a book about science fiction", this is analogous to Query. The library uses the description of each book (Key) in the library that is similar to the customers query to recommend books that fit the genre of science fiction and provides the list of these books to the customer (Value).</p>
<p>Queries, Keys, and Values are computed as linear transformations of the input embeddings (or outputs of the previous layer):</p>
<p>$$
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
$$</p>
<p>where <span class="arithmatex">\(X\)</span> is the input, and <span class="arithmatex">\(W_Q\)</span>, <span class="arithmatex">\(W_K\)</span>, <span class="arithmatex">\(W_V\)</span> are learned weight matrices.</p>
<h3 id="summary"><a class="toclink" href="../../2025/01/11/all-about-attention/#summary">Summary</a></h3>
<ol>
<li>Attention is a layer that lets a model focus on what's important</li>
<li>Query, Values and Keys are used for information retrieval insde the attention layer.</li>
</ol>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://avatars.githubusercontent.com/u/39479720?s=400&u=17af60674de436a0ef97d3e528947194708b03bd&v=4" alt="Alton Lavin D'souza">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2025-01-09 00:00:00+00:00">January 9, 2025</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../artificial-neural-networks/" class="md-meta__link">Artificial Neural Networks</a>, 
              <a href="./" class="md-meta__link">Artificial Intelligence</a>, 
              <a href="../llms/" class="md-meta__link">LLMs</a>, 
              <a href="../vlms/" class="md-meta__link">VLMs</a>, 
              <a href="../llava/" class="md-meta__link">LLaVA</a></li>
        
        
          
          <li class="md-meta__item">
            
              2 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="llava"><a class="toclink" href="../../2025/01/09/llava/">LLaVA</a></h2>
<h3 id="overview"><a class="toclink" href="../../2025/01/09/llava/#overview">Overview</a></h3>
<p>LLaVA (Large Language and Vision Assistant) was first introduced in the paper "Visual Instruction Tuning".</p>
<h3 id="what-is-visual-instruction-tuning"><a class="toclink" href="../../2025/01/09/llava/#what-is-visual-instruction-tuning">What is Visual Instruction Tuning?</a></h3>
<p>Visual instruction tuning is a method used to fine-tune a large language model, enabling it to interpret and respond to instructions derived from visual inputs.</p>
<p>One example is to ask a machine learning model to describe an image.</p>
<h3 id="llava_1"><a class="toclink" href="../../2025/01/09/llava/#llava_1">LLaVA</a></h3>
<p>As already established LLaVA is a multimodal model. LLaVA was trained on a small dataset. Despite this it can perform image analysis and respond to questions.</p>
<h4 id="architecture"><a class="toclink" href="../../2025/01/09/llava/#architecture">Architecture</a></h4>
<p>The LLaVA has the following components:
1. Language model
2. Vision Encoder
3. Projection</p>
<p>We use the Llama as the language model, which is a family of autoregressive LLMs released by Meta AI.</p>
<p>The vision encoder is implemented by CLIP visual encoder ViT-L/14. The encoder extracts visual features and connects them to language embeddings through a projection matrix. The projection component translates visual features into language embedding tokens, thereby bridgin the gap between text and images.</p>
<h4 id="training"><a class="toclink" href="../../2025/01/09/llava/#training">Training</a></h4>
<p>Two stages of training:</p>
<ol>
<li>Pre-training for Feature Alignment: LLaVA aligns visual and language features to ensure compatibility in this initial stage.</li>
<li>Fine-tune end-to-end: The second training stage focuses on fine-tuning the entire model. At this stage the vision encoder's weights remain fixed</li>
</ol>
<h3 id="llava-15"><a class="toclink" href="../../2025/01/09/llava/#llava-15">LLaVA-1.5</a></h3>
<p>In LLaVA-1.5 there are two significant changes:
1. MLP vision-language connector
2. Trained for academic task-oriented data.</p>
<p>The linear projection layer is replaced with a 2 layer MLP.</p>
<h3 id="llava-16-llava-next"><a class="toclink" href="../../2025/01/09/llava/#llava-16-llava-next">LLaVA 1.6 (LLaVA-NeXT)</a></h3>
<p>n addition to LLaVA 1.5, which uses the Vicuna-1.5 (7B and 13B) LLM backbone, LLaVA 1.6 considers more LLMs, including Mistral-7B and Nous-Hermes-2-Yi-34B. These LLMs possess nice properties, flexible commercial use terms, strong bilingual support, and a larger language model capacity. It allows LLaVA to support a broader spectrum of users and more scenarios in the community. The LLaVA recipe works well with various LLMs and scales up smoothly with the LLM up to 34B.</p>
<p>Here are the performance improvements LLaVA-NeXT has over LLaVA-1.5:</p>
<p>Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, and 1344x336 resolution.
Better visual reasoning and zero-shot OCR capability with multimodal document and chart data.
Improved visual instruction tuning data mixture with a higher diversity of task instructions and optimizing for responses that solicit favorable user feedback.
Better visual conversation for more scenarios covering different applications. Better world knowledge and logical reasoning.
Efficient deployment and inference with SGLang.</p>
<p>Other variants of LLaVA:
1. LLaVA-Med
2. LLaVA-Interactive</p>
<h3 id="reference"><a class="toclink" href="../../2025/01/09/llava/#reference">Reference</a></h3>
<ol>
<li>A. Acharya, “LLAVA, LLAVA-1.5, and LLAVA-NeXT(1.6) explained,” Nov. 04, 2024. https://encord.com/blog/llava-large-language-vision-assistant/</li>
<li>Wikipedia contributors, “Llama (language model),” Wikipedia, Jan. 01, 2025. https://en.wikipedia.org/wiki/Llama_(language_model)</li>
</ol>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://avatars.githubusercontent.com/u/39479720?s=400&u=17af60674de436a0ef97d3e528947194708b03bd&v=4" alt="Alton Lavin D'souza">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2025-01-09 00:00:00+00:00">January 9, 2025</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../artificial-neural-networks/" class="md-meta__link">Artificial Neural Networks</a>, 
              <a href="./" class="md-meta__link">Artificial Intelligence</a>, 
              <a href="../llms/" class="md-meta__link">LLMs</a>, 
              <a href="../hugging-face/" class="md-meta__link">Hugging Face</a></li>
        
        
          
          <li class="md-meta__item">
            
              2 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="introduction-to-hugging-face"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/">Introduction to Hugging Face</a></h2>
<h3 id="overview"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#overview">Overview</a></h3>
<p>Hugging Face is a leading platform in natural language processing (NLP) and machine learning (ML), providing tools, libraries, and models for developers and researchers. It is widely known for its open-source libraries and community contributions, facilitating the use of pre-trained models and accelerating ML workflows.</p>
<h4 id="applications-of-hugging-face"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#applications-of-hugging-face">Applications of Hugging Face:</a></h4>
<ul>
<li>Sentiment Analysis</li>
<li>Text Summarization</li>
<li>Machine Translation</li>
<li>Chatbots and Virtual Assistants</li>
<li>Image Captioning (via VLMs)</li>
<li>Healthcare, legal, and financial domain-specific NLP solutions</li>
</ul>
<h4 id="why-hugging-face-matters"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#why-hugging-face-matters">Why Hugging Face Matters:</a></h4>
<p>Hugging Face democratizes access to advanced AI tools, fostering innovation and collaboration. With its open-source ethos, it has become a go-to resource for researchers and developers alike, empowering them to tackle complex challenges in AI and ML effectively.</p>
<p>Hugging Face can be used with both TensorFlow and PyTorch.</p>
<h3 id="hugging-face-autoclasses"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#hugging-face-autoclasses">Hugging Face AutoClasses</a></h3>
<p>Hugging Face AutoClasses are an abstraction that simplifies the use of pre-trained models for various tasks, such as text classification, translation, and summarization. They automatically select the appropriate architecture and configuration for a given pre-trained model from the Hugging Face Model Hub.</p>
<h4 id="commonly-used-autoclasses"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#commonly-used-autoclasses">Commonly Used AutoClasses:</a></h4>
<h5 id="1-automodel"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#1-automodel">1. <strong><code>AutoModel</code></strong></a></h5>
<ul>
<li>For loading generic pre-trained models.</li>
<li>Use case: Extracting hidden states or embeddings.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="2-automodelforsequenceclassification"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#2-automodelforsequenceclassification">2. <strong><code>AutoModelForSequenceClassification</code></strong></a></h5>
<ul>
<li>For text classification tasks.</li>
<li>Use case: Sentiment analysis, spam detection, etc.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="3-autotokenizer"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#3-autotokenizer">3. <strong><code>AutoTokenizer</code></strong></a></h5>
<ul>
<li>Automatically loads the appropriate tokenizer for the specified model.</li>
<li>Handles tokenization, encoding, and decoding.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="4-automodelforquestionanswering"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#4-automodelforquestionanswering">4. <strong><code>AutoModelForQuestionAnswering</code></strong></a></h5>
<ul>
<li>For question-answering tasks.</li>
<li>Use case: Extracting answers from context.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForQuestionAnswering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-large-uncased-whole-word-masking-finetuned-squad&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="5-automodelforseq2seqlm"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#5-automodelforseq2seqlm">5. <strong><code>AutoModelForSeq2SeqLM</code></strong></a></h5>
<ul>
<li>For sequence-to-sequence tasks like translation or summarization.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-small&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="6-automodelfortokenclassification"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#6-automodelfortokenclassification">6. <strong><code>AutoModelForTokenClassification</code></strong></a></h5>
<ul>
<li>For tasks like Named Entity Recognition (NER) or Part-of-Speech (POS) tagging.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForTokenClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="7-automodelforcausallm"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#7-automodelforcausallm">7. <strong><code>AutoModelForCausalLM</code></strong></a></h5>
<ul>
<li>For language modeling tasks that generate text.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</code></pre></div>

<h5 id="8-autoprocessor-for-multimodal-models"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#8-autoprocessor-for-multimodal-models">8. <strong><code>AutoProcessor</code></strong><strong> (for Multimodal Models)</strong></a></h5>
<ul>
<li>Loads processors for tasks involving images, text, or both.</li>
<li>Example: Vision-Language Models (e.g., CLIP).</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai/clip-vit-base-patch32&quot;</span><span class="p">)</span>
</code></pre></div>

<h4 id="use-cases-in-projects"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#use-cases-in-projects">Use Cases in Projects:</a></h4>
<ul>
<li><strong>VLMs</strong>: Use <code>AutoProcessor</code> and <code>AutoModel</code> for image-text embedding or image captioning tasks.</li>
<li><strong>Healthcare</strong>: Use <code>AutoModelForSequenceClassification</code> for text classification tasks like predicting medical conditions based on clinical notes.</li>
</ul>
<h3 id="why-use-transformers"><a class="toclink" href="../../2025/01/09/introduction-to-hugging-face/#why-use-transformers">Why use Transformers?</a></h3>
<p>Traditionally to process text we RNNS but as the window size increases we see the problem of vanishing gradients. Additionally, they are slow. Transformers are able to address these concerns.</p>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://avatars.githubusercontent.com/u/39479720?s=400&u=17af60674de436a0ef97d3e528947194708b03bd&v=4" alt="Alton Lavin D'souza">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2025-01-07 00:00:00+00:00">January 7, 2025</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../artificial-neural-networks/" class="md-meta__link">Artificial Neural Networks</a>, 
              <a href="./" class="md-meta__link">Artificial Intelligence</a>, 
              <a href="../computer-vision/" class="md-meta__link">Computer Vision</a>, 
              <a href="../image-generation/" class="md-meta__link">Image Generation</a>, 
              <a href="../stable-diffusion/" class="md-meta__link">Stable Diffusion</a></li>
        
        
          
          <li class="md-meta__item">
            
              6 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="stable-diffusion-understanding"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/">Stable Diffusion Understanding</a></h2>
<h3 id="overview"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#overview">Overview</a></h3>
<p>Stable Diffusion has become so popular for image generation. It is the go to model for developers. It is a latent diffusion model that generates AI images for text. Sometimes you can also use an image and text to generate images.</p>
<h3 id="capabilities-of-stable-diffussion"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#capabilities-of-stable-diffussion">Capabilities of Stable Diffussion</a></h3>
<p>Stable diffusion is a text-to-image model. Given a text it will produce an image.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Stable Diffusion Text to Image" src="../../pics/stable_diffusion.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 1:Basic Workflow of Stable Diffusion</em></td>
</tr>
</tbody>
</table>
<p>Stable diffusion belongs to a class of deep learning models called diffusion models. These are models that are capable of generating new data that is similar to the training data. These models are so named since they use diffusion based mechanics we see in physics. We see two types of diffusion here:
1. Forward Diffusion
2. Reverse Diffusion</p>
<h4 id="forward-diffusion"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#forward-diffusion">Forward Diffusion</a></h4>
<p>Forward diffusion is the process that adds noise to an image in steps such that it gradually becomes unrecognizable. It is similar to the process where you drop ink on tissue paper the ink eventually spreads out.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Forward Diffusion Process" src="../../pics/image.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 2: Stable diffusion Forward diffusion process taken from <a href="https://stable-diffusion-art.com/how-stable-diffusion-work/">here</a></em></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Forward Diffusion Process" src="../../pics/Analogy_pen_ink.jpg" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 3: Drop of ink from the nib of the pen  spreading on the tissue paper (AI Generated from LLama 3.2)</em></td>
</tr>
</tbody>
</table>
<h4 id="reverse-diffusion"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#reverse-diffusion">Reverse Diffusion</a></h4>
<p>Reverse diffusion is the opposite of Forward Diffusion. So rather than adding noise, it removes noise gradually from an image.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Reverse Diffusion Process" src="../../pics/reverse_diffusion_process.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 4: Stable diffusion Reverse diffusion process taken from <a href="https://stable-diffusion-art.com/how-stable-diffusion-work/">here</a></em></td>
</tr>
</tbody>
</table>
<h3 id="training-process-of-stable-diffusion"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#training-process-of-stable-diffusion">Training process of Stable Diffusion</a></h3>
<p>Adding noise is simple process and does not require explicit training. But how do get the old image back from a noisy image. We need to remove the noise from the image. To put it mathematically.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="Reverse Diffusion Process Equation" src="../../pics/Equation%20for%20Reverse%20Diffusion.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 5: Stable diffusion Reverse diffusion High Level Equation</em></td>
</tr>
</tbody>
</table>
<p>So what we need to do is predict the amount of noise that needs to be removed to produce the original almost noiseless image. We use a noise predictor which for stable diffusion is a U-net model.</p>
<h4 id="u-net-model"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#u-net-model">U-Net Model</a></h4>
<p>It is a widely used deep learning model for image segmentation. The primary purpose of the model was t o address the challenge of limited data in healthcare. This network allows you to use a smalled dataset for training while maintaining the speed and accuracy of the model.</p>
<p>The U-Net model consists of 2 paths:</p>
<ol>
<li>Contracting Path </li>
<li>Expansive Path</li>
</ol>
<p>The contracting path consist of encoders, that capture the relevant information and encode it. The expansive path contains decoders the decode the encoded information and also use the information from the contracting path via the skip connections to generate a segmentation map.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="U-net Model" src="../../pics/U-net.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 6: U-net model taken from <a href="https://www.geeksforgeeks.org/u-net-architecture-explained/">here</a></em></td>
</tr>
</tbody>
</table>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="U-net Model Encoder" src="../../pics/U-net%20Encoders.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 7: U-net model Encoder Architecture</em></td>
</tr>
</tbody>
</table>
<p></center></p>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="U-net Model Decoder" src="../../pics/U-net-decoder.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 8: U-net model Decoder Architecture</em></td>
</tr>
</tbody>
</table>
<p></center></p>
<h4 id="cost-of-running-the-model"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#cost-of-running-the-model">Cost of running the model</a></h4>
<p>Diffusion models like Google’s Imagen and Open AI’s DALL-E are in pixel space. They have used some tricks to make the model faster but still not enough. Whereas, Stable Diffusion is a latent diffusion model. Instead of operating in the high-dimensional image space, it first compresses the image into the latent space. The latent space is 48 times smaller so it reaps the benefit of crunching a lot fewer numbers. That’s why it’s a lot faster. We use a Variational Autonencoders (VAE).</p>
<p>To summarise we use U-net in the image space for faster generation we make use of the latent space, for this we use VAE. U-Net is still used as the noise predictor.</p>
<h4 id="variational-autoencoders"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#variational-autoencoders">Variational Autoencoders</a></h4>
<p>Like U-net these also have encoders and decodes, the noise is added to latent vector and is later decoded to generate the images.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="VAE overview" src="../../pics/VAE_working.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 9: VAE Working</em></td>
</tr>
</tbody>
</table>
<h4 id="does-using-latent-space-cause-loss-in-information"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#does-using-latent-space-cause-loss-in-information">Does using latent space cause loss in information?</a></h4>
<p>It might seem that while using the latent space we are loosing a lot of information, however thats not the case. It might seem that images are random but they are regular in nature. For Example: A face of any species has a mouth, ears and a nose. This is better explained by the Manifold Hypothesis.</p>
<h4 id="reverse-diffusion-in-latent-space"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#reverse-diffusion-in-latent-space">Reverse Diffusion in Latent Space</a></h4>
<p>Here’s how latent reverse diffusion in Stable Diffusion works.</p>
<ol>
<li>A random latent space matrix is generated.</li>
<li>The noise predictor estimates the noise of the latent matrix.</li>
<li>The estimated noise is then subtracted from the latent matrix.</li>
<li>Steps 2 and 3 are repeated up to specific sampling steps.</li>
<li>The decoder of VAE converts the latent matrix to the final image.</li>
</ol>
<p>The noised predictor here is still U-Net.</p>
<p>So far we have seen only image generation process which is called the unconditioned process. In the following sections we will see how we can condition for text i.e. given a text the model should generate an image.</p>
<h4 id="text-conditioning"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#text-conditioning">Text Conditioning</a></h4>
<p>To be able to generate images using the text prompts we need to perform the preprocessing steps in figure 10. In the figure the Tokenizer and Embedder are implemented by a Contrastive Language-Image Pretraining model (CLIP). It should be noted here since we are dealling with a text input the convulutional layers are replaced by cross attention layers to help establish relationship between different words in a sentence. Attention layers are the new feature extracture layers, they are going to replace RNNs and CNNs as they are faster at processing and get rid of any inductive biases due the structure of neural network.</p>
<p>There are other forms of conditioning as well</p>
<table>
<thead>
<tr>
<th style="text-align: center;"><img alt="VAE overview" src="../../pics/Stable_diffusion_text_conditioning.png" /></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><em>Figure 10: Text Conditioning steps</em></td>
</tr>
</tbody>
</table>
<h3 id="summary"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#summary">Summary</a></h3>
<p>To summarize how stable diffusion creates images here are the steps:</p>
<ol>
<li>Given a text or image we generate a random vectors in the latent space this is done through VAE encoder.</li>
<li>U-NET then predicts the noise that is added to this vector.</li>
<li>Given the amount of noise that is added we remove the noise for the latent vector.</li>
<li>Steps 2 and 3 are repeated for a certain number of sampling steps</li>
<li>Finally, the decoder of VAE converts the latent image back to pixel space. This is the image you get after running Stable Diffusion.</li>
</ol>
<h3 id="references"><a class="toclink" href="../../2025/01/07/stable-diffusion-understanding/#references">References</a></h3>
<ol>
<li>Andrew, “How does Stable Diffusion work?,” Stable Diffusion Art, Jun. 10, 2024. https://stable-diffusion-art.com/how-stable-diffusion-work/</li>
<li>GeeksforGeeks, “UNET Architecture explained,” GeeksforGeeks, Jun. 08, 2023. https://www.geeksforgeeks.org/u-net-architecture-explained/</li>
<li>O. Ronneberger, P. Fischer, and T. Brox, “U-NET: Convolutional Networks for Biomedical Image Segmentation,” arXiv.org, May 18, 2015. https://arxiv.org/abs/1505.04597</li>
<li>Wikipedia contributors, “Manifold hypothesis,” Wikipedia, Aug. 01, 2024. https://en.wikipedia.org/wiki/Manifold_hypothesis</li>
</ol>
    
  </div>
</article>
      
      
        
          



<nav class="md-pagination">
  
</nav>
        
      
    </div>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.indexes"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
      
    
  </body>
</html>