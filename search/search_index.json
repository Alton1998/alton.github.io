{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Alton Lavin D\u2019souza","text":""},{"location":"#summary","title":"Summary","text":"<p>Highly motivated and team-oriented Master\u2019s student in Computer Engineering with a strong foundation in machine learning and software development. With 3 years of proven ability to design, develop, and deploy robust software solutions, I leverage various programming languages (e.g., Java, Python) and frameworks (e.g., Terraform, PyTorch, TensorFlow). Seeking a Machine Learning Engineer or AI Software Developer position to apply my machine learning expertise to solve complex problems and contribute to the development of cutting-edge AI applications.</p>"},{"location":"#technical-skills","title":"Technical Skills","text":""},{"location":"#programming-languages","title":"Programming Languages","text":"<ul> <li>Java</li> <li>Python</li> </ul>"},{"location":"#machine-learning-frameworks","title":"Machine Learning Frameworks","text":"<ul> <li>PyTorch</li> <li>TensorFlow</li> <li>scikit-learn</li> <li>seaborn</li> <li>pandas</li> <li>numpy</li> <li>matplotlib</li> </ul>"},{"location":"#cloud-infrastructure","title":"Cloud Infrastructure","text":"<ul> <li>Oracle Cloud Infrastructure</li> <li>Amazon Web Services</li> <li>Terraform</li> </ul>"},{"location":"#software-development","title":"Software Development","text":"<ul> <li>Design Patterns</li> <li>System Design</li> <li>Backend Development</li> <li>Software Debugging</li> </ul>"},{"location":"#data-science-skills","title":"Data Science Skills","text":"<ul> <li>Data Preprocessing</li> <li>Time Series Analysis</li> <li>Visualization</li> <li>Machine Learning Development</li> </ul>"},{"location":"#developer-operations","title":"Developer Operations","text":"<ul> <li>Docker</li> <li>Jenkins</li> </ul>"},{"location":"#signal-processing","title":"Signal Processing","text":"<ul> <li>Fourier and Wavelet Transforms</li> <li>High Pass, Low Pass filter</li> <li>Band Pass and Gaussian filter</li> </ul>"},{"location":"#operating-systems","title":"Operating Systems","text":"<ul> <li>Windows</li> <li>Ubuntu</li> <li>Oracle Linux</li> </ul>"},{"location":"#other-tools","title":"Other Tools","text":"<ul> <li>Splunk</li> <li>Google Colab</li> <li>Cadence</li> <li>Maven</li> <li>Jupyter Notebook</li> <li>git</li> </ul>"},{"location":"#work-experience","title":"Work Experience","text":"<p>Oracle Cerner Level 2 Software Engineer Dec 2019 \u2013 Aug 2023 (Full Time)</p> <ul> <li>Started as an intern and was eventually promoted to Level 2 Software Engineer.</li> <li>Collaborated with team members to design, develop, maintain, review, and release software components for the Careaware EHR system.</li> <li>Developed the ALPHA Medical Driver Software Delivery system, facilitating the delivery of alpha code for medical devices to designated tenants within a cloud region.</li> <li>Improved integration testing by leveraging Docker to simulate device interactions for testing medical driver software, and created a framework around it.</li> <li>Co-designed and implemented a low-code tool that significantly reduces development time for medical device drivers.</li> <li>Created and maintained design documentation for the implemented software.</li> <li>Mentored new colleagues, facilitating a smooth onboarding experience.</li> </ul>"},{"location":"#education","title":"Education","text":"<p>Master of Engineering Electrical and Computer Engineering University of Alberta 2023 \u2013 Current</p> <p>Bachelor of Technology Computer Science and Engineering Christ University First Class 2016 \u2013 2020</p>"},{"location":"#awards","title":"Awards","text":"<p>Night Over The Town Award Oracle Cerner 2021</p>"},{"location":"#certification","title":"Certification","text":"<p>Advanced Certification in Artificial Intelligence in Digital Health and Imaging Indian Institute of Science</p> <p>This program covered various imaging tools and systems used by healthcare providers, as well as the application of different signal processing techniques to 1 and 2-dimensional signals. Furthermore, I learned how to create Deep Neural Networks to perform image and signal processing.</p>"},{"location":"#projects","title":"Projects","text":"<p>Detection of Corona Virus in Chest X-Ray Images This project involved training a CNN model to detect coronavirus in chest X-ray lung images. To achieve this, we utilized data from three separate datasets.</p> <p>Arrhythmia Detection in ECG Waveforms In this project, we explored different signal processing techniques, machine learning models, and deep learning models to detect arrhythmias in ECG signals.</p> <p>Tensor Processor in Memory This project involved the full custom design of a multiply and accumulate unit (MAC unit) for a digital integrated circuit (IC). We primarily employed static CMOS logic for the gate design, and the entire project was implemented within the Cadence design environment.</p>"},{"location":"#hobbies-and-interests","title":"Hobbies and Interests","text":"<ul> <li>Playing badminton</li> <li>Trekking and hoping to trek more in the future</li> <li>Interested in finding like-minded friends</li> <li>Hoping to break cultural barriers in Canada and interact with people from different cultures</li> </ul>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Email: altondsouza02@gmail.com</li> <li>GitHub: Alton1998</li> <li>LinkedIn: alton-dsouza-539ba1217</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/01/09/llava/","title":"LLaVA","text":""},{"location":"blog/2025/01/09/llava/#overview","title":"Overview","text":"<p>LLaVA (Large Language and Vision Assistant) was first introduced in the paper \"Visual Instruction Tuning\".</p>"},{"location":"blog/2025/01/09/llava/#what-is-visual-instruction-tuning","title":"What is Visual Instruction Tuning?","text":"<p>Visual instruction tuning is a method used to fine-tune a large language model, enabling it to interpret and respond to instructions derived from visual inputs.</p> <p>One example is to ask a machine learning model to describe an image.</p>"},{"location":"blog/2025/01/09/llava/#llava_1","title":"LLaVA","text":"<p>As already established LLaVA is a multimodal model. LLaVA was trained on a small dataset. Despite this it can perform image analysis and respond to questions.</p>"},{"location":"blog/2025/01/09/llava/#architecture","title":"Architecture","text":"<p>The LLaVA has the following components: 1. Language model 2. Vision Encoder 3. Projection</p> <p>We use the Llama as the language model, which is a family of autoregressive LLMs released by Meta AI.</p> <p>The vision encoder is implemented by CLIP visual encoder ViT-L/14. The encoder extracts visual features and connects them to language embeddings through a projection matrix. The projection component translates visual features into language embedding tokens, thereby bridgin the gap between text and images.</p>"},{"location":"blog/2025/01/09/llava/#training","title":"Training","text":"<p>Two stages of training:</p> <ol> <li>Pre-training for Feature Alignment: LLaVA aligns visual and language features to ensure compatibility in this initial stage.</li> <li>Fine-tune end-to-end: The second training stage focuses on fine-tuning the entire model. At this stage the vision encoder's weights remain fixed</li> </ol>"},{"location":"blog/2025/01/09/llava/#llava-15","title":"LLaVA-1.5","text":"<p>In LLaVA-1.5 there are two significant changes: 1. MLP vision-language connector 2. Trained for academic task-oriented data.</p> <p>The linear projection layer is replaced with a 2 layer MLP.</p>"},{"location":"blog/2025/01/09/llava/#llava-16-llava-next","title":"LLaVA 1.6 (LLaVA-NeXT)","text":"<p>n addition to LLaVA 1.5, which uses the Vicuna-1.5 (7B and 13B) LLM backbone, LLaVA 1.6 considers more LLMs, including Mistral-7B and Nous-Hermes-2-Yi-34B. These LLMs possess nice properties, flexible commercial use terms, strong bilingual support, and a larger language model capacity. It allows LLaVA to support a broader spectrum of users and more scenarios in the community. The LLaVA recipe works well with various LLMs and scales up smoothly with the LLM up to 34B.</p> <p>Here are the performance improvements LLaVA-NeXT has over LLaVA-1.5:</p> <p>Increasing the input image resolution to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, and 1344x336 resolution. Better visual reasoning and zero-shot OCR capability with multimodal document and chart data. Improved visual instruction tuning data mixture with a higher diversity of task instructions and optimizing for responses that solicit favorable user feedback. Better visual conversation for more scenarios covering different applications. Better world knowledge and logical reasoning. Efficient deployment and inference with SGLang.</p> <p>Other variants of LLaVA: 1. LLaVA-Med 2. LLaVA-Interactive</p>"},{"location":"blog/2025/01/09/llava/#reference","title":"Reference","text":"<ol> <li>A. Acharya, \u201cLLAVA, LLAVA-1.5, and LLAVA-NeXT(1.6) explained,\u201d Nov. 04, 2024. https://encord.com/blog/llava-large-language-vision-assistant/</li> <li>Wikipedia contributors, \u201cLlama (language model),\u201d Wikipedia, Jan. 01, 2025. https://en.wikipedia.org/wiki/Llama_(language_model)</li> </ol>"},{"location":"blog/2024/08/22/understanding-vision-transformers/","title":"Understanding Vision Transformers","text":""},{"location":"blog/2024/08/22/understanding-vision-transformers/#overview","title":"Overview","text":"<p>I was taking part in the ISIC 2024 challenge when I got stuck training a ResNet50 model that it started overfitting. My score at this point was 0.142. To be at the the top I had to beat the score 0.188. While scouring the internet for any new model I came across Vision Transformers. I was honestly surprised that transformer architecture could be applied to images. I came across this interesting paper called \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"</p>"},{"location":"blog/2024/08/22/understanding-vision-transformers/#about-the-paper","title":"About the Paper","text":"<p>We know that the Transformer architecture has become the norm for Natural Language Processing(NLP) tasks. Unlike in NLP tasks in conjunction with attention we also have convoluitonal networks. However, this paper demonstrates that convolitonal networks need not be applied and a pure transformer architecture applied on a sequence of image patches can perform image classification tasks really well provided that its pre-trained on large amounts of data.</p>"},{"location":"blog/2024/08/22/understanding-vision-transformers/#basic-theory-of-vision-transformers","title":"Basic Theory of Vision Transformers","text":"<p>The Vision Transformer architecture has been inspired by the successes of the Transformer successes in NLP. The first step to create a Vision Transformer is to split an image into patches. We now generate the position of these patches and then generate embeddings for them. Let us consider dimensional tranformation that is taking place here. </p> <p>Our original image X had the dimenstion HxWxC. Where H is height and W is the width of the images and C is the channel. Since, we are dealing with RGB images the C will be 3. </p> <p>After fetching the patches, we get the following dimensions NxPxPxC.</p> <p>Where N is the number of patches in an image. </p> <p>To calculate it N = \\(\\frac{H * W}{P*P}\\)</p> <p>Now, we flatten the aforementioned patches and project them via a dense layer to have a dimension D whic his known as the constant latent vector size D. Then we add the patch embeddings and the positional embeddings to retain some of the position information. The postional information is in 1D and not 2D since no performance gain was observed.</p> <p>This output is forwarded through the some layers of the transformer blocks.</p> <p>The transformer enocder block is composed of alternating layers of multiheaded self attention and MLP blocks. Layer Norm is applied before every block i.e. before an attention or MLP block and a residual connection is created after every block.</p> <p>It is to be noted that Vision Transformers have much less inductive bias than CNNs. Inductive biases are assumptions we make about a data set. For example we can assume the marks of students in a given subject to follow a gaussian distribution. CNN architectures inherently will have some biases due to the way they are structured. CNNs are structured to capture the local relationship between the pixels of an image. As CNNS get deeper the local feature extractors help tp extract the global features. In Vit only the MLP layers are local and translationally equivariant while the self attention layers are global. An hybrid version of ViT also exists where CNN is applied to extract the feature maps and then forward to the Transformer encoder block.</p>"},{"location":"blog/2024/08/22/understanding-vision-transformers/#is-it-better-than-cnns","title":"Is it better than CNNs?","text":"<p>In the paper ViT can only perform classification tasks and not Segmentation or detection tasks but it still matches or outperforms CNNs and introduces a parallelism with multihead self attention. ViT will only perform better with pre-training and requires more epochs.</p>"},{"location":"blog/2025/01/11/all-about-attention/","title":"All about attention","text":""},{"location":"blog/2025/01/11/all-about-attention/#overview","title":"Overview","text":"<p>Attention layers are now used over RNNs and even CNNs to speed up processing. In this blog we will see how attention layers are implemented.</p>"},{"location":"blog/2025/01/11/all-about-attention/#working-of-attention-layers","title":"Working of Attention layers","text":"<p>There three inputs to attention layers:</p> <ol> <li>Query: Represents the \"question\" or \"search term\" for determining which parts of the input sequence are relevant.</li> <li>Key: Represents the \"descriptor\" of each input element, used to decide how relevant each input element is to the query.</li> <li>Values: Contains the actual information or representation of each input element that will be passed along after attention is computed.</li> </ol> <p>Given a Query and Key we calculate the similarity, this allows us to use the key with the max similarity and use its value for attention.</p> \\[      \\text{Score}(Q, K) = Q \\cdot K^\\top \\] <p>The above equation results in matrix describing how much importance a query gives to a key. In the equation <code>Q</code> is the query and <code>K</code> is the key.</p> <p>The next step is scaling, we perform scaling to avoid large values, larger values require more resources for computation, So now the equation takes the following shape:</p> \\[      \\text{Scaled Score}(Q, K) = \\frac{Q \\cdot K^\\top}{\\sqrt{d_k}} \\] <p>Where \\(d_k\\) is the dimensionality of the Key vectors.</p> <p>The scores are passed through a softmax function to convert them into probabilities (attention weights). These probabilities determine the contribution of each input element. The equation now takes the following form:</p> \\[      \\text{Attention Weights} = \\text{softmax}\\left(\\frac{Q \\cdot K^\\top}{\\sqrt{d_k}}\\right) \\] <p>Overall the equation would look something like this:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V \\] Figure 1: Query Key Value <p> Figure 2: Flow of calculating Attention in Scaled Dot Product Attention Figure 2: Example mapping similar query-key value pairs <p></p> <p>Lets try to understand this with an analogy. Consider the example where you are visiting a library and ask for a book. You say \"I want a book about science fiction\", this is analogous to Query. The library uses the description of each book (Key) in the library that is similar to the customers query to recommend books that fit the genre of science fiction and provides the list of these books to the customer (Value).</p> <p>Queries, Keys, and Values are computed as linear transformations of the input embeddings (or outputs of the previous layer):   [   Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V   ]   where \\(X\\) is the input, and \\(W_Q\\), \\(W_K\\), \\(W_V\\) are learned weight matrices.</p>"},{"location":"blog/2024/08/22/gaussian-distribution-problems-in-interviews/","title":"Gaussian Distribution Problems in Interviews","text":""},{"location":"blog/2024/08/22/gaussian-distribution-problems-in-interviews/#overview","title":"Overview","text":"<p>When going for any Machine Learning or Data Science Interviews, the interviewers like to check if a candidate can model a problem after a distribution. For everyone the favourite being the Gaussian Distribution. I'm sure everyone is familiar with how to do this, but to refresh everyones memory on the subject lets look at a question.</p>"},{"location":"blog/2024/08/22/gaussian-distribution-problems-in-interviews/#interview-question","title":"Interview Question","text":"<p>On any given day the average customer visiting a store is 500 and the standard deviation is 20. What is the probability that the number of customers on any day is in the range of 480-520.</p>"},{"location":"blog/2024/08/22/gaussian-distribution-problems-in-interviews/#solution","title":"Solution","text":"<p>So lets note own the details of the problem</p> <p>We know that the mean \\(\\mu\\) is 500 and \\(\\sigma\\) is 20. Assuming gaussian distribution. Our model will look something like this:</p> <p></p> <p>So the total percentage is 68.26%</p>"},{"location":"blog/2024/08/26/computational-graphs/","title":"Computational Graphs","text":"<p>These are Directed Graphs that helps map out dependencies for mathematical computations. For Example let us consider the following set of equations:</p> <ol> <li>Y=(a-b)*(a+b)</li> <li>Let d=(a-b) and e=(a+b)</li> </ol> <p>Our dependency graph will look as follows:</p> <p></p> <p>The lower nodes are evaluated first then the higher nodes are evaluated.</p> <p>Let us consider how this works when performing chain differentiation when it comes to neural networks. </p> <p>To review chain differentiation consider the following equation:</p> <ol> <li>y = \\(u^4\\)</li> <li>u = 3x + 2 </li> </ol> <p>Performing chain rule differentiation with respect to x we would get the follolwing:</p> <p>We first perform partial differentiation of <code>u</code> with respect to <code>x</code></p> \\[\\frac{\\partial u}{\\partial x} = 3 \\] <p>Then perform partial differentiation of <code>y</code> with respect to <code>u</code></p> \\[\\frac{\\partial y}{\\partial u} = 4u^3\\] <p>Can be re-written as:</p> \\[ \\frac{\\partial y}{3\\partial x} = 4u^3\\] \\[ \\frac{\\partial y}{\\partial x} = 12 u^3\\] <p>if x = 3.0 </p> <p>u = 11</p> <p>\\(\\frac{\\partial y}{\\partial x} = 15972\\) </p> <p>Representing the above steps in a computational graph we get the following: </p> <p></p> <p>How do we implement this? Luckily this has already been implemented for us in Tensorflow and Pytorch.</p> <p>There are 2 implementations of Computational Graphs:</p> <ol> <li>Static Computational Graphs - Graphs are constructed once befor the execution of the model.</li> <li>Dynamic Computational Graphs - Graphs are constructed on the fly.</li> </ol>"},{"location":"blog/2024/08/26/computational-graphs/#tensorflow-computation-graph-implementation","title":"Tensorflow Computation Graph implementation.","text":"<pre><code>import tensorflow as tf\n</code></pre> <pre><code>2024-08-27 18:45:09.326809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-27 18:45:09.357051: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-27 18:45:09.365983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-08-27 18:45:09.395484: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</code></pre> <pre><code>x = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n    u = 3*x + 2\n    y = u ** 4\n</code></pre> <pre><code>WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1724784312.756873    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784312.767002    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784312.767097    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784312.777530    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784312.777763    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784312.777895    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784313.020475    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\nI0000 00:00:1724784313.020614    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2024-08-27 18:45:13.020636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\nI0000 00:00:1724784313.020750    1081 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\nYour kernel may have been built without NUMA support.\n2024-08-27 18:45:13.020821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n</code></pre> <pre><code>g = tape.gradient(y,x)\n</code></pre> <pre><code>g\n</code></pre> <pre><code>&lt;tf.Tensor: shape=(), dtype=float32, numpy=15972.0&gt;\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"blog/2024/08/26/computational-graphs/#pytorch-computation-graph-implementation","title":"Pytorch Computation Graph Implementation.","text":"<pre><code>import torch\n</code></pre> <pre><code>x = torch.tensor(3.0, requires_grad=True)\nu = 3*x +2\ny = u**4\n</code></pre> <pre><code>y.backward()\n</code></pre> <pre><code>x.grad\n</code></pre> <pre><code>tensor(15972.)\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"blog/2024/08/22/how-to-delete-last-5-commits-in-git/","title":"How to delete last 5 commits in git?","text":""},{"location":"blog/2024/08/22/how-to-delete-last-5-commits-in-git/#overview","title":"Overview","text":"<p>In one of my interview questions I was asked if I knew git, and was given a problem to solve with it. I was asked for a give commit how do you delete the last 5 commits. One thing to not in git there are 100 ways to do the same things so there is no right or wrong way. However, at the end of the day the interviewer decides what is right.</p>"},{"location":"blog/2024/08/22/how-to-delete-last-5-commits-in-git/#solution","title":"Solution","text":"<p>Run the following:</p> <pre><code>git reset --hard HEAD~5\n</code></pre>"},{"location":"blog/2024/08/22/how-to-delete-last-5-commits-in-git/#references","title":"References","text":"<ol> <li>Git - git-reset Documentation. (n.d.). https://git-scm.com/docs/git-reset</li> </ol>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/","title":"Introduction to Hugging Face","text":""},{"location":"blog/2025/01/09/introduction-to-hugging-face/#overview","title":"Overview","text":"<p>Hugging Face is a leading platform in natural language processing (NLP) and machine learning (ML), providing tools, libraries, and models for developers and researchers. It is widely known for its open-source libraries and community contributions, facilitating the use of pre-trained models and accelerating ML workflows.</p>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#applications-of-hugging-face","title":"Applications of Hugging Face:","text":"<ul> <li>Sentiment Analysis</li> <li>Text Summarization</li> <li>Machine Translation</li> <li>Chatbots and Virtual Assistants</li> <li>Image Captioning (via VLMs)</li> <li>Healthcare, legal, and financial domain-specific NLP solutions</li> </ul>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#why-hugging-face-matters","title":"Why Hugging Face Matters:","text":"<p>Hugging Face democratizes access to advanced AI tools, fostering innovation and collaboration. With its open-source ethos, it has become a go-to resource for researchers and developers alike, empowering them to tackle complex challenges in AI and ML effectively.</p> <p>Hugging Face can be used with both TensorFlow and PyTorch.</p>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#hugging-face-autoclasses","title":"Hugging Face AutoClasses","text":"<p>Hugging Face AutoClasses are an abstraction that simplifies the use of pre-trained models for various tasks, such as text classification, translation, and summarization. They automatically select the appropriate architecture and configuration for a given pre-trained model from the Hugging Face Model Hub.</p>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#commonly-used-autoclasses","title":"Commonly Used AutoClasses:","text":""},{"location":"blog/2025/01/09/introduction-to-hugging-face/#1-automodel","title":"1. <code>AutoModel</code>","text":"<ul> <li>For loading generic pre-trained models.</li> <li>Use case: Extracting hidden states or embeddings.</li> </ul> <pre><code>from transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#2-automodelforsequenceclassification","title":"2. <code>AutoModelForSequenceClassification</code>","text":"<ul> <li>For text classification tasks.</li> <li>Use case: Sentiment analysis, spam detection, etc.</li> </ul> <pre><code>from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#3-autotokenizer","title":"3. <code>AutoTokenizer</code>","text":"<ul> <li>Automatically loads the appropriate tokenizer for the specified model.</li> <li>Handles tokenization, encoding, and decoding.</li> </ul> <pre><code>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#4-automodelforquestionanswering","title":"4. <code>AutoModelForQuestionAnswering</code>","text":"<ul> <li>For question-answering tasks.</li> <li>Use case: Extracting answers from context.</li> </ul> <pre><code>from transformers import AutoModelForQuestionAnswering\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#5-automodelforseq2seqlm","title":"5. <code>AutoModelForSeq2SeqLM</code>","text":"<ul> <li>For sequence-to-sequence tasks like translation or summarization.</li> </ul> <pre><code>from transformers import AutoModelForSeq2SeqLM\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#6-automodelfortokenclassification","title":"6. <code>AutoModelForTokenClassification</code>","text":"<ul> <li>For tasks like Named Entity Recognition (NER) or Part-of-Speech (POS) tagging.</li> </ul> <pre><code>from transformers import AutoModelForTokenClassification\nmodel = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#7-automodelforcausallm","title":"7. <code>AutoModelForCausalLM</code>","text":"<ul> <li>For language modeling tasks that generate text.</li> </ul> <pre><code>from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#8-autoprocessor-for-multimodal-models","title":"8. <code>AutoProcessor</code> (for Multimodal Models)","text":"<ul> <li>Loads processors for tasks involving images, text, or both.</li> <li>Example: Vision-Language Models (e.g., CLIP).</li> </ul> <pre><code>from transformers import AutoProcessor\nprocessor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n</code></pre>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#use-cases-in-projects","title":"Use Cases in Projects:","text":"<ul> <li>VLMs: Use <code>AutoProcessor</code> and <code>AutoModel</code> for image-text embedding or image captioning tasks.</li> <li>Healthcare: Use <code>AutoModelForSequenceClassification</code> for text classification tasks like predicting medical conditions based on clinical notes.</li> </ul>"},{"location":"blog/2025/01/09/introduction-to-hugging-face/#why-use-transformers","title":"Why use Transformers?","text":"<p>Traditionally to process text we RNNS but as the window size increases we see the problem of vanishing gradients. Additionally, they are slow. Transformers are able to address these concerns.</p>"},{"location":"blog/2024/05/21/langchain/","title":"LangChain","text":""},{"location":"blog/2024/05/21/langchain/#overview","title":"Overview","text":"<p>Consists of 3 components:</p> <ul> <li>Components:</li> <li>LLM Wrappers</li> <li>Prompt Templates</li> <li>Indexes for information Retrieval</li> <li>Chains: Assemble Components to solve a specific task</li> <li>Agents: allows LLMs To interact with it's environment</li> </ul>"},{"location":"blog/2024/05/21/langchain/#installation","title":"Installation","text":"<ul> <li>Use Pycharm as your preferred IDE since it makes things easier and user friendly</li> <li>Create a new project in Pycharm which looks as follows:</li> </ul>"},{"location":"blog/2024/05/21/langchain/#references","title":"References","text":"<ol> <li>Youtube</li> </ol>"},{"location":"blog/2024/08/23/hypothesis-testing/","title":"Hypothesis Testing","text":"<p>It's a statistical method used to determine whether a hypothesis about a population is true or not. It involves collection data, analyzing it, and making a decision based on a the evidence</p>"},{"location":"blog/2024/08/23/hypothesis-testing/#steps","title":"Steps","text":""},{"location":"blog/2024/08/23/hypothesis-testing/#step-1-state-your-null-and-alternate-hypothesis","title":"Step 1: State your null and alternate hypothesis","text":"<p>The null hypothesis is a prediction of no relationship between variables you are interested in. The alternate hypothesis on the other hand is your hypothesis that predicts a relationship between variables.</p>"},{"location":"blog/2024/08/23/hypothesis-testing/#examples","title":"Examples","text":"<ol> <li> <p>You want to test whether there is relationship between gender and height. Based on your knowledge of human physiology, taller than women. To test this hypothesis you restate it as:</p> </li> <li> <p>H<sub>0</sub> : Men are, on average, not taller than women</p> </li> <li>H<sub>a</sub>: Men are, on average, taller than women.</li> </ol>"},{"location":"blog/2024/08/23/hypothesis-testing/#some-guidelines-when-using-mathematical-symbols","title":"Some Guidelines when using mathematical symbols","text":"H<sub>0</sub> H<sub>a</sub> Equal (=) Not equal (\\(\\neq\\)) Greater Than or equal to (\\(\\geq\\)) Less than (\\(\\lt\\)) Less than or equal to (\\(\\leq\\)) Greater than (\\(\\gt\\))"},{"location":"blog/2024/08/23/hypothesis-testing/#examples_1","title":"Examples","text":"<p>We want to test whether the mean GPA of students in American colleges is different from 2.0.</p> <p>The null and alternative hypothese are </p> <p>H<sub>0</sub>: \\(\\mu\\) = 2.0</p> <p>H<sub>a</sub>: \\(\\mu\\) \\(\\ne\\) 2.0</p>"},{"location":"blog/2024/08/23/hypothesis-testing/#steps-2-perform-an-appropriate-statistical-test","title":"Steps 2 : Perform an appropriate statistical test","text":"<p>For this step we perform something known as the t-test. A t-test is any statistical hypothesis test in which the test statistic follows  a t-distribution under the null hypothesis.</p> <p>A t-test is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known.</p> <p>The t-test can be used to determine if the means of two sets of data are significantly different from each other.</p> <p>An independent Samples t-test compares the means for two groups. </p> <p>A paired sample t-test compares means from the same  group at different times </p> <p>A one sample t-test test the mean of a single group against a known mean.</p> <p>T-score is a ration between 2 groups and the difference within the groups.</p> <p>The larger the t score, the more difference there is between groups. </p> <p>The smaller the t score, the more similarity there is between groups.</p> <p>Every t-score has a p-value to go with it. </p> <p>A p-value is th probability that results that your sample data occured by chance.</p> <p>P-values are from 0% to 100%</p> <p>Low p-values are good. They indicate that your data did not occur by chance.</p>"},{"location":"blog/2024/08/23/hypothesis-testing/#step-3-decide-whether-to-reject-or-accept-your-null-hypothesis","title":"Step 3: Decide Whether to reject or accept your null hypothesis.","text":"<p>To understand this step let us solve a problem:</p> <p>Suppose a sample of n students were give a diagnostic test before studying a particular module and then again after completing the module. We want to find out if in general teaching leads to improvements in students knowledge/skills. We can use the results from our sample of students to draw concludsion about the impact of this module in general.</p> <p>So since we are calculating the mean of the same sample at different points in time we will be using the Pairesd t-test.</p> <p>Null hypothesis - There is no difference after completing the module. Alternate Hypothesis - There is a difference after completing the module.</p> <p>Calculate the difference between the two observations i.e. di = yi - xi.</p> <p>Calculate the mean difference d</p> <p>Calcualte the standard deviation of the differences, S<sub>d</sub> and use this to calculate the standard error of the mean difference, SE(d) = \\(\\frac{Sd}{\\sqrt{n}}\\)</p> <p>Calculate the T value</p> <p>t = \\(\\frac{d}{SE(d)}\\)</p> <p>then use a table of t value to look up the p-values for the paired t-test.</p>"},{"location":"blog/2024/08/22/plotting-several-graphs-with-a-single-line-of-code-using-matplotlib/","title":"Plotting Several graphs with a single line of code using matplotlib","text":""},{"location":"blog/2024/08/22/plotting-several-graphs-with-a-single-line-of-code-using-matplotlib/#overview","title":"Overview","text":"<p>In one of my interviews I was asked to show how to plot 2 graphs using one line of code. Unfortunately, I wasn't aware that this possibly so I told the interviewer that I knew how to do it in two lines, the interviewer just smiled. But, honestly who remembers such things, given a documentation I'm sure it would have been easy to figure it out, sadly todays interviewing strategies don't really reflect real world skills and abilities.</p>"},{"location":"blog/2024/08/22/plotting-several-graphs-with-a-single-line-of-code-using-matplotlib/#code-solution","title":"Code Solution","text":""},{"location":"blog/2024/08/22/plotting-several-graphs-with-a-single-line-of-code-using-matplotlib/#multiline-solution","title":"Multiline Solution","text":"<p>For the multiline I am sure everyone has seen and to me is the cleanest solution of all.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code># mean = 25\n# std = 10\ninput_vals =  np.random.normal(loc=25,scale=10, size=1000)\n</code></pre> <pre><code>mean = input_vals.mean()\n</code></pre> <pre><code>std = input_vals.std()\n</code></pre> <pre><code>transformed_vals = input_vals-mean\n</code></pre> <pre><code>transformed_vals = transformed_vals/std\n</code></pre> <pre><code>new_mean = 50\n</code></pre> <pre><code>new_std = 20\n</code></pre> <pre><code>new_input_vals = transformed_vals * new_std\n</code></pre> <pre><code>new_input_vals = new_input_vals + new_mean\n</code></pre> <pre><code>new_input_vals\n</code></pre> <pre><code>array([ 4.97262510e+01,  6.41950072e+01,  3.31727074e+01,  5.56440683e+01,\n        3.37928955e+01,  3.73482005e+01,  3.83536527e+01,  6.66796932e+01,\n        7.38599396e+01,  7.17467233e+01,  5.73445908e+01,  5.85044549e+01,\n        5.62885711e+01,  4.86385627e+01,  3.17002910e+01,  4.62663769e+01,\n        3.79672393e+01,  4.55757139e+01,  7.95473957e+01,  8.47314777e+01,\n        7.53494888e+01,  3.67249637e+01,  3.48962467e+01,  3.70960149e+01,\n        6.77144113e+01,  3.82444243e+01,  5.79397525e+01,  5.64606623e+01,\n        6.55255784e+01,  8.48117096e+01,  3.48664339e+01,  6.81253202e+01,\n        1.56436383e+01,  2.19698134e+01,  2.06851578e+01,  4.00919529e+01,\n        6.45135436e+01,  4.85857230e+01,  3.73739400e+01,  8.18960471e+01,\n        6.47992782e+01,  5.45578105e+01,  5.27481693e+01,  7.26556541e+01,\n        6.52766267e+01,  5.43891724e+01,  2.85271822e+01,  5.62877495e+01,\n        4.43133498e+01,  6.47283461e+01,  5.34390107e+01,  4.66288026e+01,\n        6.41413240e+01,  3.40513370e+01,  3.16913603e+00,  8.18762293e+01,\n        5.31382668e+01,  6.00845889e+01,  4.34753146e+01,  2.87196168e+01,\n        3.87930296e+01,  4.63811304e+01,  5.14380906e+01,  5.09331675e+01,\n        4.01047455e+01,  4.98966821e+01,  1.57353218e+01,  4.81616581e+01,\n        2.93790637e+01,  4.98499474e+01,  1.56813622e+01,  4.06766703e+01,\n        8.78471565e+01,  6.67271167e+01,  6.03161383e+01,  6.40485217e+01,\n        2.54927687e+01,  5.21594604e+01,  4.18997972e+01,  2.77864518e+01,\n        5.00337907e+01,  2.92284157e+01,  1.97714880e+01,  6.55726845e+01,\n        7.71951661e+01,  7.43999517e+01,  4.66773711e+01,  1.18134673e+01,\n        5.55068311e+01,  3.21075903e+01,  3.94008005e+01,  5.41726848e+01,\n        5.49448257e+01,  6.47877718e+01,  5.30595622e+01,  5.08879683e+01,\n        8.43420023e+01,  3.83604247e+01,  1.21843234e+01,  3.10478584e+01,\n        3.51744668e+01,  3.82361630e+01,  1.47947592e+01,  3.80039527e+01,\n        4.30739053e+01,  5.84298842e+01,  7.38988876e+01,  3.87285719e+01,\n        4.92386437e+01,  7.22702249e+01,  5.94864186e+01,  7.36198661e+01,\n        6.98013628e+01,  3.23705648e+01,  2.94831496e+01,  4.67371368e+01,\n        1.47115827e+01,  3.41017536e+01,  9.25220450e+01,  5.97642121e+01,\n        2.23111055e+01,  4.58513839e+01,  4.75114587e+01,  6.09169697e+01,\n        8.96741991e+01,  2.74447414e+01,  3.24948919e+01,  8.65286916e+00,\n        6.32930859e+01,  7.45680651e+01,  3.59225163e+01,  5.71493098e+01,\n        8.94081094e+01,  7.02884584e+01,  5.38496261e+01,  7.07504111e+01,\n        3.04737645e+01,  3.72031734e+01,  3.03921084e+01,  5.00790002e+01,\n        6.12979878e+01,  6.53701184e+01,  5.77009139e+01,  4.91265147e+01,\n        3.68800412e+01,  4.82236334e+01,  4.37972056e+01,  5.56659721e+01,\n        2.08395038e+01,  5.50958362e+01,  3.78893588e+01,  5.82563020e+01,\n        4.20204618e+01,  4.79336410e+01,  6.52104373e+01,  4.58605376e+01,\n        3.96606379e+01,  4.12586878e+01,  4.24242992e+01,  5.41711562e+01,\n        6.82626827e+01,  5.01087719e+01,  5.48861417e+01,  5.17354191e+01,\n        6.95877431e+01,  7.29643914e+01,  1.59792948e+01,  6.44605155e+01,\n        4.36807351e+01,  1.74546933e+01,  2.15064512e+01,  9.94782836e+01,\n        1.24698053e+01,  4.40146170e+01,  7.27866215e+01,  1.18408753e+02,\n        4.53963544e+01,  3.75532874e+01,  7.47684629e+01,  3.25432418e+01,\n        4.82419479e+01,  3.77868317e+01,  4.59511787e+01,  3.01010155e+01,\n        8.25522728e+01,  3.13445806e+01,  7.40198978e+01,  4.47147954e+01,\n        2.91605249e+01,  7.70967373e+01,  3.28211397e+01,  6.94203122e+01,\n        4.62309477e+01,  8.96018151e+01,  4.32844940e+01,  4.44285282e+01,\n        4.01668092e+01,  5.59602076e+01,  4.23566550e+01,  3.65968598e+01,\n        1.14229872e+01,  3.71482985e+01,  8.01091294e+01,  4.26600040e+01,\n        3.83182886e+01,  3.24364629e+01,  6.30436659e+01,  2.76899255e+01,\n        2.22386780e+01,  3.64829703e+01,  4.60831499e+01,  1.14579604e+01,\n        4.46950083e+01,  1.00299143e+02,  5.74519355e+01,  6.13887878e+01,\n        5.90151114e+01,  2.22913660e+01,  4.61747723e+01,  2.81523472e+01,\n        4.48447243e+01,  4.87802270e+01,  6.42045734e+01,  3.03457615e+01,\n        8.83800053e+01,  4.77675112e+01,  7.25055015e+01,  5.19353632e+01,\n        2.40197645e+01,  3.61380929e+01,  7.13512344e+01,  5.21470790e+01,\n        7.69906031e+01,  2.75694021e+01,  7.23380117e+01,  1.95939793e+01,\n        3.45883787e+01,  4.74206251e+01,  7.34629071e+01,  4.62772912e+01,\n        5.27110697e+01,  5.14255840e+01,  3.96448349e+01,  3.92314837e+01,\n        5.66604813e+01,  5.81154220e+01,  8.49046159e+01,  5.78898135e+01,\n        7.47500642e+01,  5.42751552e+01,  3.17079483e+01,  6.70735969e+01,\n        4.26900907e+01,  4.72223307e+01,  5.96336708e+01,  4.73144992e+01,\n        5.19295141e+01,  3.78976423e+01,  4.71163070e+01,  4.28080968e+01,\n        6.74578499e+01,  3.36385231e+01,  4.56653620e+01,  5.98283524e+01,\n        4.75220460e+01,  4.79692643e+01,  8.56734504e+01,  4.23672947e+01,\n        3.85615637e+01,  5.94519284e+01,  5.16635569e+01,  5.49745706e+01,\n        1.31747054e+01,  6.39176194e+01,  4.05638556e+00,  6.88252478e+01,\n        3.80693041e+01,  2.22358353e+01,  3.77655707e+01,  7.08921778e+01,\n        5.22594965e+01,  7.28271419e+01,  7.08960460e+01,  4.18381183e+01,\n        7.60904140e+01,  7.75712748e+01,  5.25571895e+01,  3.45138621e+01,\n        5.90094221e+01,  4.65788325e+01,  5.29888300e+01,  5.85780875e+01,\n        5.52777096e+01,  5.39039751e+01,  1.91815228e+01,  2.71299695e+01,\n        2.72143762e+01,  5.80603010e+01,  6.56306427e+01,  8.25692629e+01,\n        5.05455022e+01,  5.10168834e+01,  7.59587958e+01,  7.07376843e+01,\n        7.77485431e+01,  4.76461076e+01,  6.26839262e+01,  7.53709810e+01,\n        7.66992457e+01,  7.86190359e+01,  4.11500387e+01,  6.46650079e+01,\n        6.75092945e+01,  6.18657855e+01,  3.47069066e+01,  4.90315732e+01,\n        5.59901125e+01, -7.13206907e+00,  3.90779801e+01,  5.21050554e+01,\n        6.05622883e+01,  5.89984192e+01,  5.11460405e+01,  1.02582766e+02,\n        2.89821361e+01,  4.91031112e+01,  4.40159464e+01,  4.74407985e+01,\n        5.58102999e+01,  2.93277082e+01,  3.93095491e+01,  5.47872697e+01,\n        5.14029703e+01,  4.58367135e+01,  8.30366453e+01,  2.07225809e+01,\n        6.31351680e+01,  6.15225721e+01,  5.86669909e+01,  4.18155907e+01,\n        5.06449622e+01,  4.14820891e+01,  4.97943141e+01,  3.79254761e+01,\n        7.54749511e+01,  6.17251789e+01,  1.42164277e+01,  7.30519857e+01,\n        3.42574031e+01,  4.15106984e+01,  6.72139686e+01,  5.70645493e+01,\n        4.54665387e+01,  6.39127614e+01,  6.26129839e+01,  7.00442025e+00,\n        8.09706085e+01,  7.43156548e+01,  6.12057299e+01,  6.60028997e+01,\n        2.18863815e+01,  4.35482627e+01,  5.38520469e+01,  3.95945988e+01,\n        7.60195700e+01,  4.76573024e+01,  7.07293677e+01,  4.21540174e+01,\n        4.48038890e+01,  4.85922153e+01,  8.53943034e+01,  2.12740534e+01,\n        4.51812300e+01,  5.88532344e+01,  5.08892988e+01,  5.83297631e+01,\n        5.40651366e+01,  9.53071181e+01,  3.32160564e+01,  6.67088840e+01,\n        5.05602708e+01,  2.67525713e+01,  5.66178909e+01,  5.89578580e+01,\n        6.31056322e+01,  4.45047275e+01,  7.77652947e+01,  1.65665943e+01,\n        9.21281336e+00,  6.70786566e+01,  7.89790898e+01,  5.27143671e+01,\n        6.55934057e+01,  3.29242919e+01,  1.37965581e+01,  6.70400724e+01,\n        4.92815947e+01,  3.44505910e+01,  5.68040067e+01,  9.37290408e+01,\n        6.09953027e+01,  8.38328886e+00,  6.20682087e+01,  4.31342902e+01,\n        7.25334904e+01,  5.72517092e+01,  4.76387444e+01,  3.80033722e+01,\n        5.04482360e+01,  2.96231687e+01,  1.34192831e-01,  8.87831485e+01,\n        1.41383927e+01,  7.32963162e+01,  7.13482698e+01,  5.87271065e+01,\n        5.89149905e+01,  5.43743618e+01,  2.33782645e+01,  8.73338154e+01,\n        3.89354036e+01,  8.31282293e+01,  5.10178835e+01,  4.68211778e+01,\n        6.74195746e+01,  6.86139959e+01,  4.49257539e+01,  7.08247597e+01,\n        4.77463767e+01,  4.27463813e+01,  5.48333853e+01,  4.56286212e+01,\n        7.37659954e+01,  6.76497431e+01,  7.43349252e+01,  2.74369610e+01,\n        2.08726313e+01,  2.93575378e+01,  9.12203968e+01,  3.29439074e+01,\n        5.11289798e+01,  4.57862012e+01,  5.23203585e+01,  5.94027517e+01,\n        2.42369032e+00,  4.00518867e+01,  7.54936277e+01,  5.07548038e+01,\n        8.25171646e+01,  7.36683698e+01,  2.77424516e+01,  4.75824588e+01,\n        5.55655132e+01,  5.70535471e+01,  6.67019729e+00,  3.92544149e+01,\n        6.10040650e+01,  8.60116283e+01,  2.19124240e+01,  6.61397950e+01,\n        7.14626210e+01,  4.09508847e+01,  2.53556091e+01,  8.28124456e+01,\n        7.22639930e+01,  6.57439935e+01,  6.33561310e+01,  3.78015732e+01,\n        6.72807038e+01,  4.55893441e+01,  3.54098818e+01,  9.14033959e+01,\n        5.77696118e+01,  1.73361121e+01,  3.55965579e+01,  4.89605535e+01,\n        4.37579688e+01,  2.41240079e+01,  5.63656456e+01,  5.40022076e+01,\n        5.89146713e+01,  3.54158996e+01,  3.13192480e+01,  5.09523492e+01,\n        7.29536084e+01,  1.17849418e+01,  7.20372819e+01,  4.13907780e+01,\n        5.30141806e+01,  4.79106341e+01,  5.01993383e+01,  6.64080819e+01,\n        3.69450646e+01,  7.80334238e+01,  1.03843232e+01,  6.06052153e+01,\n        8.70960677e+01,  4.01089777e+01,  3.78049049e+01,  2.52034547e+01,\n        4.99666275e+01,  1.00503437e+01,  4.53602907e+01,  3.09624754e+01,\n       -7.99140676e-02,  4.46377557e+01,  7.64994366e+00,  6.07883609e+01,\n        6.70382744e+01,  6.70079346e+01,  1.42662635e+01,  2.58376760e+01,\n        4.49155830e+01,  4.38667674e+01,  5.99132865e+01,  9.03995003e+01,\n        6.72256253e+01,  4.66440304e+01,  6.42204993e+01,  7.29842018e+01,\n        3.63530880e+01,  6.92127579e+01,  2.25541133e+01,  5.60455480e+01,\n        3.36509265e+01,  3.53390073e+01,  5.23129106e+01,  6.83742016e+01,\n        1.08261980e+01,  5.02824005e+01,  7.69441160e+00,  8.05087956e+01,\n        6.22867329e+01,  4.80514310e+01,  8.41161530e+01,  4.37407917e+01,\n        3.91530075e+01,  4.12052603e+01,  4.67602607e+01,  5.56884932e+01,\n        6.22239594e+01,  2.83791169e+01,  3.24140023e+01,  4.85493183e+01,\n        4.93584225e+01,  8.62252700e+00,  2.09977812e+01,  5.67277542e+01,\n        5.96250735e+01,  5.74181738e+01,  4.43996392e+01,  4.30414447e+01,\n        5.84483853e+01,  7.66382827e+01, -4.85587934e+00,  6.51916200e+01,\n        2.88079069e+01,  4.80334253e+01,  1.70666462e+01,  6.43579081e+01,\n        8.50358951e+01,  2.75791679e+01,  6.53467623e+01,  1.35252106e+01,\n        6.04285460e+01,  3.52335648e+01,  5.66689933e+01,  3.70492809e+01,\n        5.12373052e+01,  3.60483097e+01,  4.49377604e+01,  3.62568237e+01,\n        7.21569769e+01,  5.45280619e+01,  4.62863317e+01,  2.15721111e+01,\n        6.28020262e+01,  7.75276147e+01,  2.58502699e+00,  5.00169680e+01,\n        4.75507661e+01,  4.55487987e+01,  3.84815627e+01,  3.28662067e+01,\n        5.76735884e+01,  6.23575473e+01,  3.68540728e+01,  4.25834012e+01,\n        5.19736971e+01,  3.90329786e+01,  3.72490570e+01,  3.58506327e+01,\n        4.19682204e+01,  7.44048682e+01,  4.73775205e+00,  8.33666390e+01,\n        5.90984994e+01,  9.94508265e+01,  6.92984708e+01,  1.27482587e+01,\n        5.06972841e+01,  3.59086858e+01,  2.13352086e+01,  4.59703349e+01,\n        6.02407192e+01,  4.90976392e+01,  2.53531858e+01,  5.54674832e+01,\n        4.20822661e+01,  3.49161297e+01,  4.16385198e+01,  3.72166837e+01,\n        4.05985426e+01,  1.07587588e+02,  6.88183705e+01,  4.80585968e+01,\n        6.50940544e+01,  8.79890694e+01,  1.72163266e+01,  6.27449264e+01,\n        7.89349884e+01,  5.36553511e+01,  4.15373357e+01,  1.95405270e+01,\n        6.11897996e+01,  5.48447329e+01,  3.96444936e+01,  3.45375731e+01,\n        7.39040149e+01,  6.03592172e+01,  2.98847636e+01,  7.33700978e+01,\n        4.47181400e+01,  3.04585534e+01,  9.59927625e+00,  8.29373699e+01,\n        5.52836836e+01,  9.81207909e+00,  6.86285567e+01,  4.77446949e+01,\n        5.76675372e+01,  5.37974812e+01,  5.55379880e+01,  6.02699173e+01,\n        6.67370555e+01,  7.54287953e+01,  4.59787102e+01,  4.60474407e+01,\n        6.38744624e+01,  4.39079068e+01,  2.05678197e+01,  6.27469865e+01,\n        5.79685123e+01,  5.58725664e+01,  4.12867606e+01,  5.60118119e+01,\n        3.35720315e+01,  4.50343744e+01,  8.03815346e+01,  1.42108638e+01,\n        8.04637036e+01,  6.59676991e+01,  5.45857009e+01,  5.74991413e+01,\n        6.82651475e+01,  1.40455660e+01,  4.38444823e+01,  4.81339086e+01,\n        3.33897656e+01,  5.91857305e+01,  7.82359957e+01,  5.21267627e+01,\n        1.57856710e+01,  4.62251109e+01,  5.87693791e+01,  3.38576994e+01,\n        4.21268689e+01,  3.83494154e+01,  3.81119768e+01,  5.45086177e+01,\n        5.30471482e+01,  2.03662756e+01,  4.90384811e+01,  5.85900308e+01,\n        5.07983431e+01,  4.82232284e+01,  3.57633314e+01,  3.88925452e+01,\n        5.19604565e+01,  1.90800527e+01,  6.13642399e+01,  2.99467151e+01,\n        4.47324900e+01,  2.45477723e+01,  7.12581742e+01,  6.77801891e+01,\n        3.58031105e+01,  4.54614741e+01,  5.70220249e+01,  3.90174595e+01,\n        6.02058825e+01,  5.20856895e+01,  4.75049498e+01,  3.86241729e+01,\n        6.13581051e+01,  4.19909369e+01,  6.76175905e+01,  7.42983906e+01,\n        7.92241492e+01,  5.33594593e+01,  4.63789503e+01,  3.35663333e+01,\n        4.01872677e+01,  8.36656134e+01,  8.16212747e+01,  6.93496119e+01,\n        4.61663697e+01,  3.73110654e+01,  5.49127377e+01,  4.40223836e+01,\n        4.43108572e+01,  6.75982282e+01,  3.48185881e+01,  5.77048872e+01,\n        3.43615266e+01,  5.96583647e+01,  3.83303854e+01,  6.44889437e+01,\n        8.88984256e+01,  6.05667673e+01,  2.46517283e+01,  5.35936489e+01,\n        6.83251874e+01,  7.50465482e+01,  5.35890343e+01,  5.71776979e+01,\n        4.71590330e+01,  4.29138167e+01,  3.18721166e+01,  1.78723665e+01,\n        2.37579224e+01,  4.70345771e+01,  4.98658045e+01,  7.95162899e+01,\n        3.01964307e+01,  3.61605063e+01,  6.48481042e+01,  5.82651006e+01,\n        5.67299061e+01,  5.82468645e+01,  5.27659064e+01,  7.80144548e+01,\n       -1.81875982e+00,  4.48166231e+01,  2.44844292e+01,  5.00175138e+01,\n        4.23655802e+01,  3.42827337e+01,  6.79546054e+01,  9.19132017e+01,\n        4.01333503e+01,  5.97016053e+01,  6.53891457e+01,  6.49021729e+01,\n        5.10671254e+01,  6.55272499e+01,  5.11000031e+01,  9.88725349e+01,\n        3.58597844e+01,  7.74396710e+01,  4.92101330e+01,  5.11284270e+01,\n        5.05615107e+01,  8.56510102e+01,  5.44176611e+01,  5.28692453e+01,\n        4.48416859e+01,  6.96655859e+01,  6.66747245e+01,  2.58324302e+01,\n        4.58193524e+01,  2.12910503e+01,  3.34128789e+01,  7.27412851e+01,\n        9.30526713e+01,  7.86137249e+01,  6.24940419e+01,  8.65207000e+01,\n        5.41363809e+01,  4.43530597e+01,  4.82412920e+01,  6.18822743e+01,\n        1.48342484e+01,  4.09532013e+01,  7.47863553e+01,  6.88263728e+01,\n        4.26618101e+01,  4.73684324e+01,  2.55504883e+01,  3.68092107e+01,\n        3.05964151e+01,  2.92729438e+01,  7.68143135e+01,  2.00636758e+01,\n        3.93172214e+01,  7.12013201e+01,  5.31840226e+01,  4.33205088e+01,\n        1.85507408e+01,  6.93950937e+01,  4.08774379e+01,  7.53997880e+01,\n        3.33300308e+01,  6.12127659e+01,  7.47711766e+01,  6.59695680e+01,\n        6.48252715e+01,  4.51280419e+01,  7.34813321e+01,  9.91452566e+01,\n        4.06925312e+01,  4.82945436e+01,  3.85368371e+01,  7.94166760e+01,\n        7.61656778e+01,  3.12710381e+01,  6.72961529e+01,  4.13614225e+01,\n        2.25621381e+01,  1.98036372e+01,  3.44802122e+01,  6.26084073e+01,\n        3.91710105e+01,  3.10526458e+01,  5.10588932e+01,  4.90629575e+01,\n        1.57885874e+01,  4.86224457e+01,  3.62146642e+01,  6.32971028e+01,\n        5.06452397e+01,  3.06098599e+01,  4.46450667e+01,  1.27694720e+01,\n        7.13258569e+01,  3.22340451e+01,  7.69632817e+01,  3.33908566e+01,\n        5.80107954e+01,  5.58820102e+01,  3.06086354e+01,  6.17342669e+01,\n        5.82069831e+01,  5.44247348e+01,  4.11739065e+01,  4.91914253e+01,\n        1.03889391e+02,  4.46818900e+01,  6.39503129e+01,  3.87421151e+01,\n        5.37844463e+01,  1.79796161e+01,  6.94401545e+01,  3.99892484e+01,\n        7.85746916e+01,  5.62061154e+01,  2.71297470e+01,  2.07495010e+01,\n        9.18238208e+01,  4.30828868e+00,  5.89973284e+01,  3.69521887e+01,\n        7.77100538e+01,  2.91855571e+01,  4.39590658e+01,  6.59981463e+01,\n        1.54102818e+01,  6.17457410e+01,  5.76508387e+01,  6.77781768e+01,\n        6.49134137e+01,  3.04790867e+01,  1.97005788e+01,  4.47335344e+01,\n        5.73058793e+01,  2.59902769e+01,  6.29930422e+01,  4.40924417e+01,\n        4.68671957e+00,  5.33445891e+01,  5.86015222e+01,  2.32235267e+01,\n        4.54584938e+01,  1.31758595e+01,  5.65895862e+01,  6.69366623e+01,\n        2.76825617e+01,  7.99193476e+00,  4.30404663e+01,  4.23856129e+01,\n        6.06713182e+01,  8.35082535e+01,  4.94483630e+01,  3.58838838e+01,\n        9.04293255e+01,  5.49940920e+01,  7.93762465e+01,  3.51502094e+01,\n        5.74676491e+01,  6.87092366e+01,  3.59306135e+01,  4.86862467e+01,\n        6.06140023e+01,  3.95157652e+01,  2.93690746e+01,  2.14781414e+00,\n        5.97178956e+01,  6.02090846e+01,  3.97506829e+01,  7.82891107e+01,\n        4.25032054e+01,  4.00644889e+01,  3.19230837e+01,  3.78505725e+01,\n        7.06475423e+01,  4.31800544e+01,  4.82657650e+01,  8.05206693e+01,\n        4.31868963e+01,  4.85046034e+01,  4.21593980e+01,  1.51990681e+01,\n        3.53738859e+01,  8.01984118e+01,  7.64656001e+01,  5.45685869e+01,\n        5.13807080e+01,  8.29278438e+01,  5.83801566e+01,  5.74714255e+01,\n        5.72713761e+01,  5.07381142e+01,  2.18652001e+01,  3.33351646e+01,\n        2.02119171e+01,  7.34906719e+01,  7.98721720e+01,  7.94965018e+01,\n        1.00811358e+02,  3.71707949e+01,  3.35557355e+01,  3.38923918e+01,\n        6.85451700e+01,  9.81232875e+01,  4.84287819e+01,  3.99635350e+01,\n        6.26134916e+01,  4.12477016e+01,  7.22482149e+01,  3.51448266e+01,\n        6.11000646e+01,  4.51660178e+01,  2.39193305e+01,  3.28749845e+01,\n        2.12626130e+01,  8.69176784e+01,  4.13164819e+01,  3.16309606e+01,\n        8.75437651e+01,  3.26831039e+01,  6.16947213e+01,  6.29633742e+01,\n        3.04670647e+01,  3.60134661e+01,  4.32430529e+01,  3.09289105e+01,\n        5.93031665e+01,  2.52868794e+01,  2.20828915e+01,  7.68208071e+01,\n        9.80676336e+01,  3.83871338e+01,  3.68649160e+01,  1.76748364e+01,\n        1.35316383e+01,  5.03074303e+01,  3.39946035e+01,  5.43990016e+01,\n        2.09051370e+01,  6.23807961e+01,  3.91031121e+01,  5.57380667e+01,\n        4.49007304e+01,  4.28668359e+01,  5.74135957e+01,  7.79245845e+01,\n        3.05567507e+01,  5.01540564e+01,  3.92652109e+01,  6.05587289e+01])\n</code></pre> <pre><code>new_input_vals.mean()\n</code></pre> <pre><code>50.0\n</code></pre> <pre><code>new_input_vals.std()\n</code></pre> <pre><code>20.0\n</code></pre> <pre><code>plt.plot(np.arange(0,1000),input_vals,\"r--\")\nplt.plot(np.arange(0,1000),new_input_vals,\"g--\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7f9e64f3c650&gt;]\n</code></pre> <p></p> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"blog/2024/08/22/plotting-several-graphs-with-a-single-line-of-code-using-matplotlib/#one-line-solution","title":"One line Solution","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code># mean = 25\n# std = 10\ninput_vals =  np.random.normal(loc=25,scale=10, size=1000)\n</code></pre> <pre><code>mean = input_vals.mean()\n</code></pre> <pre><code>std = input_vals.std()\n</code></pre> <pre><code>transformed_vals = input_vals-mean\n</code></pre> <pre><code>transformed_vals = transformed_vals/std\n</code></pre> <pre><code>new_mean = 50\n</code></pre> <pre><code>new_std = 20\n</code></pre> <pre><code>new_input_vals = transformed_vals * new_std\n</code></pre> <pre><code>new_input_vals = new_input_vals + new_mean\n</code></pre> <pre><code>new_input_vals\n</code></pre> <pre><code>array([ 4.97262510e+01,  6.41950072e+01,  3.31727074e+01,  5.56440683e+01,\n        3.37928955e+01,  3.73482005e+01,  3.83536527e+01,  6.66796932e+01,\n        7.38599396e+01,  7.17467233e+01,  5.73445908e+01,  5.85044549e+01,\n        5.62885711e+01,  4.86385627e+01,  3.17002910e+01,  4.62663769e+01,\n        3.79672393e+01,  4.55757139e+01,  7.95473957e+01,  8.47314777e+01,\n        7.53494888e+01,  3.67249637e+01,  3.48962467e+01,  3.70960149e+01,\n        6.77144113e+01,  3.82444243e+01,  5.79397525e+01,  5.64606623e+01,\n        6.55255784e+01,  8.48117096e+01,  3.48664339e+01,  6.81253202e+01,\n        1.56436383e+01,  2.19698134e+01,  2.06851578e+01,  4.00919529e+01,\n        6.45135436e+01,  4.85857230e+01,  3.73739400e+01,  8.18960471e+01,\n        6.47992782e+01,  5.45578105e+01,  5.27481693e+01,  7.26556541e+01,\n        6.52766267e+01,  5.43891724e+01,  2.85271822e+01,  5.62877495e+01,\n        4.43133498e+01,  6.47283461e+01,  5.34390107e+01,  4.66288026e+01,\n        6.41413240e+01,  3.40513370e+01,  3.16913603e+00,  8.18762293e+01,\n        5.31382668e+01,  6.00845889e+01,  4.34753146e+01,  2.87196168e+01,\n        3.87930296e+01,  4.63811304e+01,  5.14380906e+01,  5.09331675e+01,\n        4.01047455e+01,  4.98966821e+01,  1.57353218e+01,  4.81616581e+01,\n        2.93790637e+01,  4.98499474e+01,  1.56813622e+01,  4.06766703e+01,\n        8.78471565e+01,  6.67271167e+01,  6.03161383e+01,  6.40485217e+01,\n        2.54927687e+01,  5.21594604e+01,  4.18997972e+01,  2.77864518e+01,\n        5.00337907e+01,  2.92284157e+01,  1.97714880e+01,  6.55726845e+01,\n        7.71951661e+01,  7.43999517e+01,  4.66773711e+01,  1.18134673e+01,\n        5.55068311e+01,  3.21075903e+01,  3.94008005e+01,  5.41726848e+01,\n        5.49448257e+01,  6.47877718e+01,  5.30595622e+01,  5.08879683e+01,\n        8.43420023e+01,  3.83604247e+01,  1.21843234e+01,  3.10478584e+01,\n        3.51744668e+01,  3.82361630e+01,  1.47947592e+01,  3.80039527e+01,\n        4.30739053e+01,  5.84298842e+01,  7.38988876e+01,  3.87285719e+01,\n        4.92386437e+01,  7.22702249e+01,  5.94864186e+01,  7.36198661e+01,\n        6.98013628e+01,  3.23705648e+01,  2.94831496e+01,  4.67371368e+01,\n        1.47115827e+01,  3.41017536e+01,  9.25220450e+01,  5.97642121e+01,\n        2.23111055e+01,  4.58513839e+01,  4.75114587e+01,  6.09169697e+01,\n        8.96741991e+01,  2.74447414e+01,  3.24948919e+01,  8.65286916e+00,\n        6.32930859e+01,  7.45680651e+01,  3.59225163e+01,  5.71493098e+01,\n        8.94081094e+01,  7.02884584e+01,  5.38496261e+01,  7.07504111e+01,\n        3.04737645e+01,  3.72031734e+01,  3.03921084e+01,  5.00790002e+01,\n        6.12979878e+01,  6.53701184e+01,  5.77009139e+01,  4.91265147e+01,\n        3.68800412e+01,  4.82236334e+01,  4.37972056e+01,  5.56659721e+01,\n        2.08395038e+01,  5.50958362e+01,  3.78893588e+01,  5.82563020e+01,\n        4.20204618e+01,  4.79336410e+01,  6.52104373e+01,  4.58605376e+01,\n        3.96606379e+01,  4.12586878e+01,  4.24242992e+01,  5.41711562e+01,\n        6.82626827e+01,  5.01087719e+01,  5.48861417e+01,  5.17354191e+01,\n        6.95877431e+01,  7.29643914e+01,  1.59792948e+01,  6.44605155e+01,\n        4.36807351e+01,  1.74546933e+01,  2.15064512e+01,  9.94782836e+01,\n        1.24698053e+01,  4.40146170e+01,  7.27866215e+01,  1.18408753e+02,\n        4.53963544e+01,  3.75532874e+01,  7.47684629e+01,  3.25432418e+01,\n        4.82419479e+01,  3.77868317e+01,  4.59511787e+01,  3.01010155e+01,\n        8.25522728e+01,  3.13445806e+01,  7.40198978e+01,  4.47147954e+01,\n        2.91605249e+01,  7.70967373e+01,  3.28211397e+01,  6.94203122e+01,\n        4.62309477e+01,  8.96018151e+01,  4.32844940e+01,  4.44285282e+01,\n        4.01668092e+01,  5.59602076e+01,  4.23566550e+01,  3.65968598e+01,\n        1.14229872e+01,  3.71482985e+01,  8.01091294e+01,  4.26600040e+01,\n        3.83182886e+01,  3.24364629e+01,  6.30436659e+01,  2.76899255e+01,\n        2.22386780e+01,  3.64829703e+01,  4.60831499e+01,  1.14579604e+01,\n        4.46950083e+01,  1.00299143e+02,  5.74519355e+01,  6.13887878e+01,\n        5.90151114e+01,  2.22913660e+01,  4.61747723e+01,  2.81523472e+01,\n        4.48447243e+01,  4.87802270e+01,  6.42045734e+01,  3.03457615e+01,\n        8.83800053e+01,  4.77675112e+01,  7.25055015e+01,  5.19353632e+01,\n        2.40197645e+01,  3.61380929e+01,  7.13512344e+01,  5.21470790e+01,\n        7.69906031e+01,  2.75694021e+01,  7.23380117e+01,  1.95939793e+01,\n        3.45883787e+01,  4.74206251e+01,  7.34629071e+01,  4.62772912e+01,\n        5.27110697e+01,  5.14255840e+01,  3.96448349e+01,  3.92314837e+01,\n        5.66604813e+01,  5.81154220e+01,  8.49046159e+01,  5.78898135e+01,\n        7.47500642e+01,  5.42751552e+01,  3.17079483e+01,  6.70735969e+01,\n        4.26900907e+01,  4.72223307e+01,  5.96336708e+01,  4.73144992e+01,\n        5.19295141e+01,  3.78976423e+01,  4.71163070e+01,  4.28080968e+01,\n        6.74578499e+01,  3.36385231e+01,  4.56653620e+01,  5.98283524e+01,\n        4.75220460e+01,  4.79692643e+01,  8.56734504e+01,  4.23672947e+01,\n        3.85615637e+01,  5.94519284e+01,  5.16635569e+01,  5.49745706e+01,\n        1.31747054e+01,  6.39176194e+01,  4.05638556e+00,  6.88252478e+01,\n        3.80693041e+01,  2.22358353e+01,  3.77655707e+01,  7.08921778e+01,\n        5.22594965e+01,  7.28271419e+01,  7.08960460e+01,  4.18381183e+01,\n        7.60904140e+01,  7.75712748e+01,  5.25571895e+01,  3.45138621e+01,\n        5.90094221e+01,  4.65788325e+01,  5.29888300e+01,  5.85780875e+01,\n        5.52777096e+01,  5.39039751e+01,  1.91815228e+01,  2.71299695e+01,\n        2.72143762e+01,  5.80603010e+01,  6.56306427e+01,  8.25692629e+01,\n        5.05455022e+01,  5.10168834e+01,  7.59587958e+01,  7.07376843e+01,\n        7.77485431e+01,  4.76461076e+01,  6.26839262e+01,  7.53709810e+01,\n        7.66992457e+01,  7.86190359e+01,  4.11500387e+01,  6.46650079e+01,\n        6.75092945e+01,  6.18657855e+01,  3.47069066e+01,  4.90315732e+01,\n        5.59901125e+01, -7.13206907e+00,  3.90779801e+01,  5.21050554e+01,\n        6.05622883e+01,  5.89984192e+01,  5.11460405e+01,  1.02582766e+02,\n        2.89821361e+01,  4.91031112e+01,  4.40159464e+01,  4.74407985e+01,\n        5.58102999e+01,  2.93277082e+01,  3.93095491e+01,  5.47872697e+01,\n        5.14029703e+01,  4.58367135e+01,  8.30366453e+01,  2.07225809e+01,\n        6.31351680e+01,  6.15225721e+01,  5.86669909e+01,  4.18155907e+01,\n        5.06449622e+01,  4.14820891e+01,  4.97943141e+01,  3.79254761e+01,\n        7.54749511e+01,  6.17251789e+01,  1.42164277e+01,  7.30519857e+01,\n        3.42574031e+01,  4.15106984e+01,  6.72139686e+01,  5.70645493e+01,\n        4.54665387e+01,  6.39127614e+01,  6.26129839e+01,  7.00442025e+00,\n        8.09706085e+01,  7.43156548e+01,  6.12057299e+01,  6.60028997e+01,\n        2.18863815e+01,  4.35482627e+01,  5.38520469e+01,  3.95945988e+01,\n        7.60195700e+01,  4.76573024e+01,  7.07293677e+01,  4.21540174e+01,\n        4.48038890e+01,  4.85922153e+01,  8.53943034e+01,  2.12740534e+01,\n        4.51812300e+01,  5.88532344e+01,  5.08892988e+01,  5.83297631e+01,\n        5.40651366e+01,  9.53071181e+01,  3.32160564e+01,  6.67088840e+01,\n        5.05602708e+01,  2.67525713e+01,  5.66178909e+01,  5.89578580e+01,\n        6.31056322e+01,  4.45047275e+01,  7.77652947e+01,  1.65665943e+01,\n        9.21281336e+00,  6.70786566e+01,  7.89790898e+01,  5.27143671e+01,\n        6.55934057e+01,  3.29242919e+01,  1.37965581e+01,  6.70400724e+01,\n        4.92815947e+01,  3.44505910e+01,  5.68040067e+01,  9.37290408e+01,\n        6.09953027e+01,  8.38328886e+00,  6.20682087e+01,  4.31342902e+01,\n        7.25334904e+01,  5.72517092e+01,  4.76387444e+01,  3.80033722e+01,\n        5.04482360e+01,  2.96231687e+01,  1.34192831e-01,  8.87831485e+01,\n        1.41383927e+01,  7.32963162e+01,  7.13482698e+01,  5.87271065e+01,\n        5.89149905e+01,  5.43743618e+01,  2.33782645e+01,  8.73338154e+01,\n        3.89354036e+01,  8.31282293e+01,  5.10178835e+01,  4.68211778e+01,\n        6.74195746e+01,  6.86139959e+01,  4.49257539e+01,  7.08247597e+01,\n        4.77463767e+01,  4.27463813e+01,  5.48333853e+01,  4.56286212e+01,\n        7.37659954e+01,  6.76497431e+01,  7.43349252e+01,  2.74369610e+01,\n        2.08726313e+01,  2.93575378e+01,  9.12203968e+01,  3.29439074e+01,\n        5.11289798e+01,  4.57862012e+01,  5.23203585e+01,  5.94027517e+01,\n        2.42369032e+00,  4.00518867e+01,  7.54936277e+01,  5.07548038e+01,\n        8.25171646e+01,  7.36683698e+01,  2.77424516e+01,  4.75824588e+01,\n        5.55655132e+01,  5.70535471e+01,  6.67019729e+00,  3.92544149e+01,\n        6.10040650e+01,  8.60116283e+01,  2.19124240e+01,  6.61397950e+01,\n        7.14626210e+01,  4.09508847e+01,  2.53556091e+01,  8.28124456e+01,\n        7.22639930e+01,  6.57439935e+01,  6.33561310e+01,  3.78015732e+01,\n        6.72807038e+01,  4.55893441e+01,  3.54098818e+01,  9.14033959e+01,\n        5.77696118e+01,  1.73361121e+01,  3.55965579e+01,  4.89605535e+01,\n        4.37579688e+01,  2.41240079e+01,  5.63656456e+01,  5.40022076e+01,\n        5.89146713e+01,  3.54158996e+01,  3.13192480e+01,  5.09523492e+01,\n        7.29536084e+01,  1.17849418e+01,  7.20372819e+01,  4.13907780e+01,\n        5.30141806e+01,  4.79106341e+01,  5.01993383e+01,  6.64080819e+01,\n        3.69450646e+01,  7.80334238e+01,  1.03843232e+01,  6.06052153e+01,\n        8.70960677e+01,  4.01089777e+01,  3.78049049e+01,  2.52034547e+01,\n        4.99666275e+01,  1.00503437e+01,  4.53602907e+01,  3.09624754e+01,\n       -7.99140676e-02,  4.46377557e+01,  7.64994366e+00,  6.07883609e+01,\n        6.70382744e+01,  6.70079346e+01,  1.42662635e+01,  2.58376760e+01,\n        4.49155830e+01,  4.38667674e+01,  5.99132865e+01,  9.03995003e+01,\n        6.72256253e+01,  4.66440304e+01,  6.42204993e+01,  7.29842018e+01,\n        3.63530880e+01,  6.92127579e+01,  2.25541133e+01,  5.60455480e+01,\n        3.36509265e+01,  3.53390073e+01,  5.23129106e+01,  6.83742016e+01,\n        1.08261980e+01,  5.02824005e+01,  7.69441160e+00,  8.05087956e+01,\n        6.22867329e+01,  4.80514310e+01,  8.41161530e+01,  4.37407917e+01,\n        3.91530075e+01,  4.12052603e+01,  4.67602607e+01,  5.56884932e+01,\n        6.22239594e+01,  2.83791169e+01,  3.24140023e+01,  4.85493183e+01,\n        4.93584225e+01,  8.62252700e+00,  2.09977812e+01,  5.67277542e+01,\n        5.96250735e+01,  5.74181738e+01,  4.43996392e+01,  4.30414447e+01,\n        5.84483853e+01,  7.66382827e+01, -4.85587934e+00,  6.51916200e+01,\n        2.88079069e+01,  4.80334253e+01,  1.70666462e+01,  6.43579081e+01,\n        8.50358951e+01,  2.75791679e+01,  6.53467623e+01,  1.35252106e+01,\n        6.04285460e+01,  3.52335648e+01,  5.66689933e+01,  3.70492809e+01,\n        5.12373052e+01,  3.60483097e+01,  4.49377604e+01,  3.62568237e+01,\n        7.21569769e+01,  5.45280619e+01,  4.62863317e+01,  2.15721111e+01,\n        6.28020262e+01,  7.75276147e+01,  2.58502699e+00,  5.00169680e+01,\n        4.75507661e+01,  4.55487987e+01,  3.84815627e+01,  3.28662067e+01,\n        5.76735884e+01,  6.23575473e+01,  3.68540728e+01,  4.25834012e+01,\n        5.19736971e+01,  3.90329786e+01,  3.72490570e+01,  3.58506327e+01,\n        4.19682204e+01,  7.44048682e+01,  4.73775205e+00,  8.33666390e+01,\n        5.90984994e+01,  9.94508265e+01,  6.92984708e+01,  1.27482587e+01,\n        5.06972841e+01,  3.59086858e+01,  2.13352086e+01,  4.59703349e+01,\n        6.02407192e+01,  4.90976392e+01,  2.53531858e+01,  5.54674832e+01,\n        4.20822661e+01,  3.49161297e+01,  4.16385198e+01,  3.72166837e+01,\n        4.05985426e+01,  1.07587588e+02,  6.88183705e+01,  4.80585968e+01,\n        6.50940544e+01,  8.79890694e+01,  1.72163266e+01,  6.27449264e+01,\n        7.89349884e+01,  5.36553511e+01,  4.15373357e+01,  1.95405270e+01,\n        6.11897996e+01,  5.48447329e+01,  3.96444936e+01,  3.45375731e+01,\n        7.39040149e+01,  6.03592172e+01,  2.98847636e+01,  7.33700978e+01,\n        4.47181400e+01,  3.04585534e+01,  9.59927625e+00,  8.29373699e+01,\n        5.52836836e+01,  9.81207909e+00,  6.86285567e+01,  4.77446949e+01,\n        5.76675372e+01,  5.37974812e+01,  5.55379880e+01,  6.02699173e+01,\n        6.67370555e+01,  7.54287953e+01,  4.59787102e+01,  4.60474407e+01,\n        6.38744624e+01,  4.39079068e+01,  2.05678197e+01,  6.27469865e+01,\n        5.79685123e+01,  5.58725664e+01,  4.12867606e+01,  5.60118119e+01,\n        3.35720315e+01,  4.50343744e+01,  8.03815346e+01,  1.42108638e+01,\n        8.04637036e+01,  6.59676991e+01,  5.45857009e+01,  5.74991413e+01,\n        6.82651475e+01,  1.40455660e+01,  4.38444823e+01,  4.81339086e+01,\n        3.33897656e+01,  5.91857305e+01,  7.82359957e+01,  5.21267627e+01,\n        1.57856710e+01,  4.62251109e+01,  5.87693791e+01,  3.38576994e+01,\n        4.21268689e+01,  3.83494154e+01,  3.81119768e+01,  5.45086177e+01,\n        5.30471482e+01,  2.03662756e+01,  4.90384811e+01,  5.85900308e+01,\n        5.07983431e+01,  4.82232284e+01,  3.57633314e+01,  3.88925452e+01,\n        5.19604565e+01,  1.90800527e+01,  6.13642399e+01,  2.99467151e+01,\n        4.47324900e+01,  2.45477723e+01,  7.12581742e+01,  6.77801891e+01,\n        3.58031105e+01,  4.54614741e+01,  5.70220249e+01,  3.90174595e+01,\n        6.02058825e+01,  5.20856895e+01,  4.75049498e+01,  3.86241729e+01,\n        6.13581051e+01,  4.19909369e+01,  6.76175905e+01,  7.42983906e+01,\n        7.92241492e+01,  5.33594593e+01,  4.63789503e+01,  3.35663333e+01,\n        4.01872677e+01,  8.36656134e+01,  8.16212747e+01,  6.93496119e+01,\n        4.61663697e+01,  3.73110654e+01,  5.49127377e+01,  4.40223836e+01,\n        4.43108572e+01,  6.75982282e+01,  3.48185881e+01,  5.77048872e+01,\n        3.43615266e+01,  5.96583647e+01,  3.83303854e+01,  6.44889437e+01,\n        8.88984256e+01,  6.05667673e+01,  2.46517283e+01,  5.35936489e+01,\n        6.83251874e+01,  7.50465482e+01,  5.35890343e+01,  5.71776979e+01,\n        4.71590330e+01,  4.29138167e+01,  3.18721166e+01,  1.78723665e+01,\n        2.37579224e+01,  4.70345771e+01,  4.98658045e+01,  7.95162899e+01,\n        3.01964307e+01,  3.61605063e+01,  6.48481042e+01,  5.82651006e+01,\n        5.67299061e+01,  5.82468645e+01,  5.27659064e+01,  7.80144548e+01,\n       -1.81875982e+00,  4.48166231e+01,  2.44844292e+01,  5.00175138e+01,\n        4.23655802e+01,  3.42827337e+01,  6.79546054e+01,  9.19132017e+01,\n        4.01333503e+01,  5.97016053e+01,  6.53891457e+01,  6.49021729e+01,\n        5.10671254e+01,  6.55272499e+01,  5.11000031e+01,  9.88725349e+01,\n        3.58597844e+01,  7.74396710e+01,  4.92101330e+01,  5.11284270e+01,\n        5.05615107e+01,  8.56510102e+01,  5.44176611e+01,  5.28692453e+01,\n        4.48416859e+01,  6.96655859e+01,  6.66747245e+01,  2.58324302e+01,\n        4.58193524e+01,  2.12910503e+01,  3.34128789e+01,  7.27412851e+01,\n        9.30526713e+01,  7.86137249e+01,  6.24940419e+01,  8.65207000e+01,\n        5.41363809e+01,  4.43530597e+01,  4.82412920e+01,  6.18822743e+01,\n        1.48342484e+01,  4.09532013e+01,  7.47863553e+01,  6.88263728e+01,\n        4.26618101e+01,  4.73684324e+01,  2.55504883e+01,  3.68092107e+01,\n        3.05964151e+01,  2.92729438e+01,  7.68143135e+01,  2.00636758e+01,\n        3.93172214e+01,  7.12013201e+01,  5.31840226e+01,  4.33205088e+01,\n        1.85507408e+01,  6.93950937e+01,  4.08774379e+01,  7.53997880e+01,\n        3.33300308e+01,  6.12127659e+01,  7.47711766e+01,  6.59695680e+01,\n        6.48252715e+01,  4.51280419e+01,  7.34813321e+01,  9.91452566e+01,\n        4.06925312e+01,  4.82945436e+01,  3.85368371e+01,  7.94166760e+01,\n        7.61656778e+01,  3.12710381e+01,  6.72961529e+01,  4.13614225e+01,\n        2.25621381e+01,  1.98036372e+01,  3.44802122e+01,  6.26084073e+01,\n        3.91710105e+01,  3.10526458e+01,  5.10588932e+01,  4.90629575e+01,\n        1.57885874e+01,  4.86224457e+01,  3.62146642e+01,  6.32971028e+01,\n        5.06452397e+01,  3.06098599e+01,  4.46450667e+01,  1.27694720e+01,\n        7.13258569e+01,  3.22340451e+01,  7.69632817e+01,  3.33908566e+01,\n        5.80107954e+01,  5.58820102e+01,  3.06086354e+01,  6.17342669e+01,\n        5.82069831e+01,  5.44247348e+01,  4.11739065e+01,  4.91914253e+01,\n        1.03889391e+02,  4.46818900e+01,  6.39503129e+01,  3.87421151e+01,\n        5.37844463e+01,  1.79796161e+01,  6.94401545e+01,  3.99892484e+01,\n        7.85746916e+01,  5.62061154e+01,  2.71297470e+01,  2.07495010e+01,\n        9.18238208e+01,  4.30828868e+00,  5.89973284e+01,  3.69521887e+01,\n        7.77100538e+01,  2.91855571e+01,  4.39590658e+01,  6.59981463e+01,\n        1.54102818e+01,  6.17457410e+01,  5.76508387e+01,  6.77781768e+01,\n        6.49134137e+01,  3.04790867e+01,  1.97005788e+01,  4.47335344e+01,\n        5.73058793e+01,  2.59902769e+01,  6.29930422e+01,  4.40924417e+01,\n        4.68671957e+00,  5.33445891e+01,  5.86015222e+01,  2.32235267e+01,\n        4.54584938e+01,  1.31758595e+01,  5.65895862e+01,  6.69366623e+01,\n        2.76825617e+01,  7.99193476e+00,  4.30404663e+01,  4.23856129e+01,\n        6.06713182e+01,  8.35082535e+01,  4.94483630e+01,  3.58838838e+01,\n        9.04293255e+01,  5.49940920e+01,  7.93762465e+01,  3.51502094e+01,\n        5.74676491e+01,  6.87092366e+01,  3.59306135e+01,  4.86862467e+01,\n        6.06140023e+01,  3.95157652e+01,  2.93690746e+01,  2.14781414e+00,\n        5.97178956e+01,  6.02090846e+01,  3.97506829e+01,  7.82891107e+01,\n        4.25032054e+01,  4.00644889e+01,  3.19230837e+01,  3.78505725e+01,\n        7.06475423e+01,  4.31800544e+01,  4.82657650e+01,  8.05206693e+01,\n        4.31868963e+01,  4.85046034e+01,  4.21593980e+01,  1.51990681e+01,\n        3.53738859e+01,  8.01984118e+01,  7.64656001e+01,  5.45685869e+01,\n        5.13807080e+01,  8.29278438e+01,  5.83801566e+01,  5.74714255e+01,\n        5.72713761e+01,  5.07381142e+01,  2.18652001e+01,  3.33351646e+01,\n        2.02119171e+01,  7.34906719e+01,  7.98721720e+01,  7.94965018e+01,\n        1.00811358e+02,  3.71707949e+01,  3.35557355e+01,  3.38923918e+01,\n        6.85451700e+01,  9.81232875e+01,  4.84287819e+01,  3.99635350e+01,\n        6.26134916e+01,  4.12477016e+01,  7.22482149e+01,  3.51448266e+01,\n        6.11000646e+01,  4.51660178e+01,  2.39193305e+01,  3.28749845e+01,\n        2.12626130e+01,  8.69176784e+01,  4.13164819e+01,  3.16309606e+01,\n        8.75437651e+01,  3.26831039e+01,  6.16947213e+01,  6.29633742e+01,\n        3.04670647e+01,  3.60134661e+01,  4.32430529e+01,  3.09289105e+01,\n        5.93031665e+01,  2.52868794e+01,  2.20828915e+01,  7.68208071e+01,\n        9.80676336e+01,  3.83871338e+01,  3.68649160e+01,  1.76748364e+01,\n        1.35316383e+01,  5.03074303e+01,  3.39946035e+01,  5.43990016e+01,\n        2.09051370e+01,  6.23807961e+01,  3.91031121e+01,  5.57380667e+01,\n        4.49007304e+01,  4.28668359e+01,  5.74135957e+01,  7.79245845e+01,\n        3.05567507e+01,  5.01540564e+01,  3.92652109e+01,  6.05587289e+01])\n</code></pre> <pre><code>new_input_vals.mean()\n</code></pre> <pre><code>50.0\n</code></pre> <pre><code>new_input_vals.std()\n</code></pre> <pre><code>20.0\n</code></pre> <pre><code>plt.plot(np.arange(0,1000),input_vals,\"r--\",np.arange(0,1000),new_input_vals,\"g--\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7f9e64fb5150&gt;,\n &lt;matplotlib.lines.Line2D at 0x7f9e64fb5250&gt;]\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/","title":"Stable Diffusion Understanding","text":""},{"location":"blog/2025/01/07/stable-diffusion-understanding/#overview","title":"Overview","text":"<p>Stable Diffusion has become so popular for image generation. It is the go to model for developers. It is a latent diffusion model that generates AI images for text. Sometimes you can also use an image and text to generate images.</p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#capabilities-of-stable-diffussion","title":"Capabilities of Stable Diffussion","text":"<p>Stable diffusion is a text-to-image model. Given a text it will produce an image.</p> Figure 1:Basic Workflow of Stable Diffusion <p>Stable diffusion belongs to a class of deep learning models called diffusion models. These are models that are capable of generating new data that is similar to the training data. These models are so named since they use diffusion based mechanics we see in physics. We see two types of diffusion here: 1. Forward Diffusion 2. Reverse Diffusion</p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#forward-diffusion","title":"Forward Diffusion","text":"<p>Forward diffusion is the process that adds noise to an image in steps such that it gradually becomes unrecognizable. It is similar to the process where you drop ink on tissue paper the ink eventually spreads out.</p> Figure 2: Stable diffusion Forward diffusion process taken from here Figure 3: Drop of ink from the nib of the pen  spreading on the tissue paper (AI Generated from LLama 3.2)"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#reverse-diffusion","title":"Reverse Diffusion","text":"<p>Reverse diffusion is the opposite of Forward Diffusion. So rather than adding noise, it removes noise gradually from an image.</p> Figure 4: Stable diffusion Reverse diffusion process taken from here"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#training-process-of-stable-diffusion","title":"Training process of Stable Diffusion","text":"<p>Adding noise is simple process and does not require explicit training. But how do get the old image back from a noisy image. We need to remove the noise from the image. To put it mathematically.</p> Figure 5: Stable diffusion Reverse diffusion High Level Equation <p>So what we need to do is predict the amount of noise that needs to be removed to produce the original almost noiseless image. We use a noise predictor which for stable diffusion is a U-net model.</p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#u-net-model","title":"U-Net Model","text":"<p>It is a widely used deep learning model for image segmentation. The primary purpose of the model was t o address the challenge of limited data in healthcare. This network allows you to use a smalled dataset for training while maintaining the speed and accuracy of the model.</p> <p>The U-Net model consists of 2 paths:</p> <ol> <li>Contracting Path </li> <li>Expansive Path</li> </ol> <p>The contracting path consist of encoders, that capture the relevant information and encode it. The expansive path contains decoders the decode the encoded information and also use the information from the contracting path via the skip connections to generate a segmentation map.</p> Figure 6: U-net model taken from here <p> Figure 7: U-net model Encoder Architecture <p></p> <p> Figure 8: U-net model Decoder Architecture <p></p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#cost-of-running-the-model","title":"Cost of running the model","text":"<p>Diffusion models like Google\u2019s Imagen and Open AI\u2019s DALL-E are in pixel space. They have used some tricks to make the model faster but still not enough. Whereas, Stable Diffusion is a latent diffusion model. Instead of operating in the high-dimensional image space, it first compresses the image into the latent space. The latent space is 48 times smaller so it reaps the benefit of crunching a lot fewer numbers. That\u2019s why it\u2019s a lot faster. We use a Variational Autonencoders (VAE).</p> <p>To summarise we use U-net in the image space for faster generation we make use of the latent space, for this we use VAE. U-Net is still used as the noise predictor.</p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#variational-autoencoders","title":"Variational Autoencoders","text":"<p>Like U-net these also have encoders and decodes, the noise is added to latent vector and is later decoded to generate the images.</p> Figure 9: VAE Working"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#does-using-latent-space-cause-loss-in-information","title":"Does using latent space cause loss in information?","text":"<p>It might seem that while using the latent space we are loosing a lot of information, however thats not the case. It might seem that images are random but they are regular in nature. For Example: A face of any species has a mouth, ears and a nose. This is better explained by the Manifold Hypothesis.</p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#reverse-diffusion-in-latent-space","title":"Reverse Diffusion in Latent Space","text":"<p>Here\u2019s how latent reverse diffusion in Stable Diffusion works.</p> <ol> <li>A random latent space matrix is generated.</li> <li>The noise predictor estimates the noise of the latent matrix.</li> <li>The estimated noise is then subtracted from the latent matrix.</li> <li>Steps 2 and 3 are repeated up to specific sampling steps.</li> <li>The decoder of VAE converts the latent matrix to the final image.</li> </ol> <p>The noised predictor here is still U-Net.</p> <p>So far we have seen only image generation process which is called the unconditioned process. In the following sections we will see how we can condition for text i.e. given a text the model should generate an image.</p>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#text-conditioning","title":"Text Conditioning","text":"<p>To be able to generate images using the text prompts we need to perform the preprocessing steps in figure 10. In the figure the Tokenizer and Embedder are implemented by a Contrastive Language-Image Pretraining model (CLIP). It should be noted here since we are dealling with a text input the convulutional layers are replaced by cross attention layers to help establish relationship between different words in a sentence. Attention layers are the new feature extracture layers, they are going to replace RNNs and CNNs as they are faster at processing and get rid of any inductive biases due the structure of neural network.</p> <p>There are other forms of conditioning as well</p> Figure 10: Text Conditioning steps"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#summary","title":"Summary","text":"<p>To summarize how stable diffusion creates images here are the steps:</p> <ol> <li>Given a text or image we generate a random vectors in the latent space this is done through VAE encoder.</li> <li>U-NET then predicts the noise that is added to this vector.</li> <li>Given the amount of noise that is added we remove the noise for the latent vector.</li> <li>Steps 2 and 3 are repeated for a certain number of sampling steps</li> <li>Finally, the decoder of VAE converts the latent image back to pixel space. This is the image you get after running Stable Diffusion.</li> </ol>"},{"location":"blog/2025/01/07/stable-diffusion-understanding/#references","title":"References","text":"<ol> <li>Andrew, \u201cHow does Stable Diffusion work?,\u201d Stable Diffusion Art, Jun. 10, 2024. https://stable-diffusion-art.com/how-stable-diffusion-work/</li> <li>GeeksforGeeks, \u201cUNET Architecture explained,\u201d GeeksforGeeks, Jun. 08, 2023. https://www.geeksforgeeks.org/u-net-architecture-explained/</li> <li>O. Ronneberger, P. Fischer, and T. Brox, \u201cU-NET: Convolutional Networks for Biomedical Image Segmentation,\u201d arXiv.org, May 18, 2015. https://arxiv.org/abs/1505.04597</li> <li>Wikipedia contributors, \u201cManifold hypothesis,\u201d Wikipedia, Aug. 01, 2024. https://en.wikipedia.org/wiki/Manifold_hypothesis</li> </ol>"},{"location":"blog/2024/08/22/transforming-data-to-new-mean-and-variance/","title":"Transforming Data to New Mean and Variance","text":""},{"location":"blog/2024/08/22/transforming-data-to-new-mean-and-variance/#overview","title":"Overview","text":"<p>Sometimes, we would want to transform a data to new mean or variance for different purposes. For instance, for a normal distribution we might want to skew the mean and variance to correct an error during data collection. These operations are very common in data wrangling</p>"},{"location":"blog/2024/08/22/transforming-data-to-new-mean-and-variance/#how-do-you-do-it","title":"How do you do it?","text":"<p>It's quite simple really. Let us understand the math behind it.</p>"},{"location":"blog/2024/08/22/transforming-data-to-new-mean-and-variance/#step-1-standardize-your-dataset","title":"Step 1: Standardize your dataset","text":"<p>To standardise your dataset you need to perform the following transformation first:</p> \\[ z = \\frac{x - \\mu}{\\sigma} \\] <p>Where x is the input \\(\\mu\\) is the meand and \\(\\sigma\\) is the standard deviation and \\(\\sigma^2\\) is the variance. This implementation is similar to the sklearn implementation for <code>StandardScaler</code> in python.</p>"},{"location":"blog/2024/08/22/transforming-data-to-new-mean-and-variance/#step-2-calculating-new-inputs","title":"Step 2: Calculating new inputs","text":"<p>To calculate the new inputs we simply need to use the following formula:</p> \\[ x' = z*\\sigma' + \\mu' \\] <p>Where x' is the new input values and \\(\\sigma'\\) is the new standard deviation and \\(\\mu'\\) is the new mean</p>"},{"location":"blog/2024/08/22/transforming-data-to-new-mean-and-variance/#python-code","title":"Python Code","text":"<pre><code>import numpy as np\n</code></pre> <pre><code># mean = 25\n# std = 10\ninput_vals =  np.random.normal(loc=25,scale=10, size=1000)\n</code></pre> <pre><code>mean = input_vals.mean()\n</code></pre> <pre><code>std = input_vals.std()\n</code></pre> <pre><code>transformed_vals = input_vals-mean\n</code></pre> <pre><code>transformed_vals = transformed_vals/std\n</code></pre> <pre><code>new_mean = 50\n</code></pre> <pre><code>new_std = 20\n</code></pre> <pre><code>new_input_vals = transformed_vals * new_std\n</code></pre> <pre><code>new_input_vals = new_input_vals + new_mean\n</code></pre> <pre><code>new_input_vals\n</code></pre> <pre><code>array([ 49.49601559,  81.95221737,  44.10701235,  -4.43684172,\n        43.12316393,  22.37556816,  50.8232128 ,  50.82211642,\n        54.34191641,  44.22147494,  47.6687756 ,  77.44097655,\n        10.03763848,  47.29017838,  32.11374724,  -1.23982667,\n        69.48691703,  48.91739126,  86.26436   ,  54.24775184,\n        -0.62913152,  46.80564143,  34.39202535,  42.49736382,\n        11.11665713,  67.84445118,  34.59465903,  61.88864806,\n        40.33390889,  51.23802672,  36.43257266,  67.12160189,\n        34.66234269,  36.87892063,   7.19051749,  32.21378979,\n        43.02836817,  43.15184828,  47.1522313 ,  42.8129976 ,\n        76.41399973,  72.86920209,  49.40598658,  20.55679545,\n        39.21964224,  41.66889385,  75.12315452,  40.55983653,\n        35.84141976,  31.58795549,  55.91554017,   2.05367405,\n        68.68371294,  89.64689468,  35.3533851 ,  65.48295692,\n        91.77368542,  66.95120604,  22.30549826,  37.0382882 ,\n        62.95677903,  52.15918057,  43.90733809,  44.98067858,\n        56.7238287 ,  32.07208405,  71.82333845,  83.68577369,\n        42.82555785,  75.28336789,  26.21175009,  96.55165242,\n        38.51350607,  70.11426895,  74.27860112,  65.36207749,\n        81.35233833,  50.18617206,  55.93360283,  77.53365166,\n        69.63520905,  37.59198146,  46.79127037,  44.60258121,\n        28.17262707,  66.3659093 ,  27.81046051,  32.12126147,\n        74.81794259,  40.02111546,  58.35644463,  72.6791059 ,\n        79.23500871,  18.61984409,  73.44601248,  62.16258075,\n        71.36050328,  72.18368902,  62.11357691,  60.01150657,\n        12.93929019,  33.25076732,  58.92842542,  85.16931205,\n        35.14074635,  30.94234057,  56.63988325,  39.54632178,\n        50.98317019,  39.01565464,  44.35046655,  42.8802156 ,\n        67.70448826,  69.68267456,  67.88673202,  47.03415948,\n        35.70957455,  62.0271697 ,  29.95210341,  85.6417443 ,\n        86.41651521,  13.35185704,  73.3740623 ,  46.06879338,\n        33.06050445,  64.62951854,  21.38971394,  36.92346644,\n        60.46378494,  56.42159419,  46.88107641,  59.5814996 ,\n        80.12756863,  44.68597813,  65.62078714,  90.01413935,\n        45.31101182,  54.69107064,  38.97076947,  32.8735822 ,\n        85.46860426,  33.63271914,  77.7963452 ,  36.70177956,\n        56.22516901,  49.61501512,  48.62277677,  34.71191839,\n        70.58431327,  47.215934  ,  86.01083296,  10.01352701,\n        47.27567932,  33.96263808,  48.70166808,  27.40983011,\n        52.55928308,  48.01592305,  61.45921988,  45.88339219,\n        79.73244761,  43.8490769 ,  63.3760365 , 108.63398782,\n        52.43253858,  44.70183207,  23.75090548,  44.57956294,\n        52.82850474,  52.25872225,  45.52727945,  52.99699568,\n        63.45042797,  74.26054273,  74.8948219 ,  57.09896442,\n        37.91847292,  89.77592224,  60.1371484 ,  24.00906182,\n        77.56627315,  81.01670238,  62.80787995,  43.67763844,\n        26.54282655,  62.64378931,  61.53516981,  56.66331484,\n        40.82505139,  35.73135155,  49.38087751,  35.38622963,\n        54.57180493,  68.1206563 ,  58.30386778,  23.94199085,\n        40.79241966,  66.08012181,  51.79244687,  22.75451046,\n        26.07074966,  33.11550218,  75.85995492,  56.79386647,\n        55.3297153 ,  30.92124425,  56.02856793,  33.96853867,\n        26.26417803,  75.88605475,  62.39167368,  34.75454549,\n        74.46107655,  37.20314954,  38.38163625,  74.66845515,\n        75.66322402,  24.28010982,  43.94214999,  40.62395058,\n        48.0843374 ,  51.98710602,  66.69022463,  83.16548506,\n        48.1024695 ,  26.16834048,  29.89578215,  60.10091581,\n        27.11641347,  53.81504542,  40.37371576,  62.53805758,\n        18.60222102,  25.46361251,  57.12387699,  20.34342658,\n        34.85790115,  29.45391819,  51.56221642,  36.93229206,\n        69.65466631,  61.23242145,  34.13827241,  46.82041056,\n        53.53046763,  34.23611329,  56.34852414,  44.12829082,\n        83.13023373,  60.70016783,  83.19769711,  32.20696478,\n        24.44920165,  61.80384099,  16.75989065,  74.25626731,\n        68.36131381,  56.16197942,  32.74899458,  59.23027229,\n        50.65085652,  46.34568657,  55.1050968 ,  71.59045805,\n        45.04037161,  48.84160109,   9.73137866,  67.13901612,\n        70.18102173,  10.97512729,  44.47815867,  58.10020807,\n        61.3599267 ,  56.59742826,  13.56102442,   3.92188211,\n        63.93193683,  74.89339799,  83.21784375,  53.03416625,\n        19.53061296,  17.03327139,  82.19633235,  55.8401601 ,\n        73.41605689,  57.66720166,   5.3398253 ,  15.02813978,\n        34.17079683,  54.57416008,  51.08909237,  59.24314076,\n         3.61556204,  48.62684436,  22.98114859,  54.50169801,\n        85.54156458,  87.04567889,  65.312347  ,  35.79250589,\n        32.26966647,  33.17354654,  47.02984857,  44.145624  ,\n        59.4074548 ,  36.55781391,  55.18682041,  58.34930214,\n        40.286385  ,  58.81879478,  12.40276423,  41.00470755,\n        53.61278918,  51.33011874,  33.00966439,  48.86525779,\n        58.35053152,  41.92292913,  38.45732378,  54.4998052 ,\n        37.50265526,  41.60417335,  38.18343955,  52.14381858,\n        51.25683047,  59.68082105,  60.96865136,  43.61793798,\n        53.45929256,  55.96906224,  23.29333053,  63.63911366,\n        23.97048848,  75.71691226,  57.50459948,  52.18680612,\n        25.88142988,  40.25965288,  18.71230929,  60.27017089,\n        37.20885774,  64.33217068, -15.80580728,  83.1248649 ,\n        23.79554506,  76.58716818,  26.2192708 ,  40.83835812,\n        23.03772042,  68.98720606,  57.57873014,  68.52695854,\n         5.14758133,  45.1179288 ,  64.97623344,  63.57160003,\n        77.08161055,  52.1084482 ,  61.68888811,  27.16821272,\n        46.0101753 ,  65.28611687,  64.46894115,  42.26620475,\n        64.90930088,  38.81286685,  67.72977263,  42.98501795,\n        44.88827085,  54.93992946,  25.47413835,  52.27312205,\n        85.69922305,  61.61658672,   7.83273776,  59.8997075 ,\n         6.73665383,  27.96350816,  70.43863418,  32.58226973,\n        48.39457149,  61.43033804,  54.35377947,  83.34789135,\n        65.9375121 ,  59.34051355,  67.24611435, 102.54055678,\n        39.02170513,  24.99161933,  43.5885292 ,  39.43727887,\n        59.12757792,  83.05789748,  47.38251863,  46.00796688,\n        65.44530126,  50.26898215,  17.59570274,  86.65436661,\n        33.2437534 ,  63.84817761,  26.69816948,  63.43081244,\n        19.51755875,  49.07459211,  41.58803545,  59.22852463,\n        63.00270275,  35.83958966,  50.24515112,  87.57937072,\n        40.74589485,  59.7137842 ,  91.7339808 ,  55.82675139,\n        65.38667423,  45.43300605,  63.62709081,  41.57023071,\n        99.50174099,  49.94879407,  85.28333911,  38.98283286,\n        22.95127191,  28.48640215,  74.40190731,  56.39152034,\n        87.11789373,  49.96592323,  47.91872821,  69.60728571,\n        51.23902791,  71.64063386,  79.6016418 ,  71.95821828,\n        77.68705163,  54.71355569,  68.26163132,   9.70179471,\n        64.29988672,  27.08808979,  73.68217633,  31.30879697,\n        44.96018459,  31.74515341,  55.90425015,  57.80316169,\n        21.13254216,  47.48109419,  54.97651116,  17.85206471,\n        48.17671051,  46.9278787 ,  31.70650285,  41.04334338,\n        54.58315842,  61.66939818,  49.1901462 ,  39.92231791,\n        62.12917698,  73.11726575,  68.64225752,  26.54410158,\n        51.51106242,  12.74469986,  39.00122347,  56.2995436 ,\n        37.5701076 ,  44.42675697,  23.99295103,  32.05722478,\n        22.74339285,  49.68311884,  59.52754907,  15.94074257,\n        50.36358847,  43.60625194,  59.88294185,  50.14999084,\n        62.31356872,  68.53996927,  38.9083538 ,  69.91116662,\n        60.54048083,  56.99174173,  46.15787547,  36.1217023 ,\n        70.14227673,  72.09617741,  83.53838605,  16.36291389,\n        63.21899147,  42.39208864,  42.11577349,  56.77338503,\n        43.29132042,  49.74369988,  11.02817352,  40.57930242,\n        78.03595312,  61.20053244,  44.25454593,  73.46078844,\n        39.19593048,  55.75033887,  82.09623613,  38.84314227,\n        40.85204792,  50.86187538,  10.72616764,  18.28888217,\n        46.55790238,  55.43780903,  38.20987205,  42.50837861,\n        79.26291754,  72.30333049,  24.20580798,  37.02655925,\n        30.89209407,  54.4846919 ,  82.50612511,  58.67607329,\n        58.23064539,  47.88389914,  18.12263489,  52.5182857 ,\n        43.41903826,  41.14214894,  68.01724688,  25.3949731 ,\n        44.7449377 ,  27.16228127,  58.73924531,  56.86747413,\n        84.61124395,  45.05992081,  71.1624996 ,  37.48260365,\n        46.48870978,  23.69901148,  51.15919288,  18.96718992,\n        56.79575079,  54.31872957,  28.83104669,  71.49908403,\n        61.81759572,  45.09595638,  85.44426864,  30.9801629 ,\n        38.49977853,  71.87824812,  59.38518561,  14.70401097,\n        23.84136401,  36.41924606,  51.85082062,  24.42709582,\n        46.96733474,  59.95369635,  87.22312558,  46.45375422,\n        79.63697279,  34.48244042,  49.61857759,  58.24109826,\n        69.28433602,  12.28545874,  36.08342907,  74.61247067,\n        44.54569835,  29.59746389,  42.73948595,  17.7863682 ,\n        34.32873818,  30.87491666,  29.31222089,  49.94580299,\n        51.12224323,  68.13000142,  55.25825597,  70.91162641,\n        39.3351263 ,  43.15723031,  48.69918162,  35.77614902,\n        48.20871546,  41.7321273 ,  56.46405943,  29.60390014,\n        51.87946251,  56.76641015,  49.74083861,  53.47655012,\n        38.35481504,  59.32787208,  64.8190974 ,   4.59899069,\n        24.65197628,  76.13535529,  31.87519699,  49.87142476,\n        55.66396191,  33.03398098,  39.32249353,  55.15723999,\n        31.75098199,  61.82868185,  67.98436359,  66.45111173,\n        68.09088384,  57.42580141,   7.90566222,  93.05117015,\n        44.21880902,  65.5422945 ,  57.44372298,  52.08799937,\n        63.14130041,  68.53298467,  48.00403173,  58.55683871,\n        59.14957095,  63.00563394,  60.46529197,  79.58424895,\n        36.21158367,  32.58760247,  55.02829998,  54.40420964,\n        58.85174303,  71.32178925,  77.24437475,  28.3589688 ,\n        32.16694145,  33.19155973,  54.26637119,  41.16327586,\n        50.58551412,  64.76908479,  38.28325466,  57.01256249,\n        26.4025322 ,  51.71212334,  58.58063136,  36.49654396,\n        45.05438892,  64.52502789,  24.11713391,  70.08197726,\n        33.83088736,  16.29646357,  57.63483978,  61.79741407,\n        59.56417407,  32.03595271,  48.93540253,  50.41790003,\n        82.64721788,  45.3544536 ,  54.81711147,  77.28636658,\n        40.19796987,  52.54404744,  32.56437429,  47.85691129,\n        58.62257712,  55.36607257,  59.41562041,  77.16077912,\n        21.38747626,  19.38870772,  85.40004097,  26.49009319,\n         8.0640901 ,  36.55693585,  14.9592918 ,  32.44420826,\n        57.87655584,  47.88871025,  80.95017661,  43.29675305,\n        54.95892607,  35.60244226,  41.27100482,  59.14702036,\n        44.85299406,  45.88970679,  64.90173338,  16.68215149,\n        60.83374325,  54.51335594,  66.12371104,  43.71407551,\n        76.35766673,  47.0578887 ,  44.48930216,  58.67214604,\n         6.93099036,  44.0784275 ,  87.26743732,  33.74051501,\n        31.61433847,  16.18837785,  71.41725892,   7.56165002,\n        74.25763798,  53.00725407,  21.21984948,  27.00235844,\n        43.27854072,  62.58751425,  72.62697027,  84.83697849,\n        39.29949741,  27.74751162,  84.54174836,  53.60907931,\n        23.51072372,  52.30666938,  44.9227771 ,  62.00104535,\n        36.76673651,  67.79146944,  76.40024822,  72.37617406,\n        32.15380912,  38.59583061,  55.06194823,  86.37161605,\n        45.8461077 ,  44.76196931,  53.67580888,  55.94391549,\n        40.54273035,  72.81303502,  80.22448102,  83.59834747,\n        42.68294477,  56.22101165,  42.50225088,  74.24983962,\n        74.41151259,  71.16334458,  41.85641042,  57.63886762,\n        53.31873729,  18.19712412,  46.10496567,  65.21976216,\n        28.71065915,  97.38021009,  32.86679772,  19.66351098,\n        38.16941953,  30.58965124, 104.90914337,  25.40905709,\n        48.50673348,  40.18356751,  40.01859325,  53.28474473,\n        15.10319505,  41.45816069,  60.98240438,  72.87696864,\n        17.83039328,  57.73843473,  69.49674981,  12.1607061 ,\n        33.19392956,  32.87282521,  22.95524749,  41.49371038,\n        21.74403892,  70.68449793,  32.62075174,  41.58195433,\n        81.18567803,  33.58930942,  34.64131154,  44.54289417,\n        64.36337003,  40.37054446,  22.78226617,  58.91188644,\n        67.67729368,  26.23094686,  62.93973918,  55.49175452,\n        51.4648093 ,  49.28916451,  47.06318444,  42.58523182,\n        56.10613689,  52.40085237,  23.38133769,  55.67721105,\n        31.49967033,  72.31546648,  59.25186389,  56.84445298,\n        66.88545026,  66.95470607,  20.38200418,  99.7359384 ,\n        46.07265114,  69.3898383 ,  78.10224603,  44.37805652,\n         8.37255714,  60.67226014,  53.11479668,  64.04126634,\n        18.04627226,  38.58411279,  56.02370156,  39.15303342,\n        34.63214408,  88.21876653,  67.44644253,  16.43229522,\n        83.90872194,  35.93661211,  60.19263761,  42.15495887,\n        58.93682819,  33.10147147,  58.65792064,  24.18783816,\n        88.72537889,  82.56698502,  49.25789954,  29.74450617,\n        58.47459962,  28.70369747,  49.89468339,  78.39481375,\n        79.47880766,  82.48991671,  58.24369182,  32.75084164,\n        54.21499264,  40.47349446,  66.56791093,  56.32661118,\n        37.41345055,  54.07731934,  91.19557085,  71.43299712,\n        34.64809948,  90.94826767,  38.70820119,  49.93275932,\n        71.52085894,  21.64971732,  67.19714009,  62.88390642,\n        11.12847965,  20.41944811,  55.73799815,  17.06247734,\n        54.17605223,  86.01987149,  53.79450487,  43.47483199,\n        67.10418026,  35.99357907,  22.21169736,  56.93503159,\n        43.87585123,  63.02749859,  33.49599091,  54.36035587,\n        39.97066494,  35.73469642,  32.5950815 ,  62.69388504,\n        55.72244848,  27.56650818,  41.4477204 ,  44.53380104,\n        86.10294567,  64.49073862,  50.72484679,  52.70817822,\n        43.01421884,  92.43937255,  70.28028032,  40.51593511,\n        34.45853813,  22.83929454,  58.21433989,  74.31262301,\n        93.80795957,   5.87029511,  53.21195362,  55.25038156,\n        72.02442023,  64.24480159,  86.85869318,  44.43041239,\n        69.01396061,  50.55803314,  56.43512514,  51.3159804 ,\n        54.75580638,  54.56525244,  55.682404  ,  74.54055696,\n        51.85127714,  54.69033416,  67.01174536,  19.75335069,\n        68.69085763,  34.59123781,  54.60158215,  25.75319982,\n        18.98062956,  56.99697019,  28.78845467,  39.70492392,\n        49.91829085,  23.6065864 ,  65.61351123,  26.89794478,\n        39.59783814,  70.30614995,  29.79739873,  44.79248053,\n        34.23500871,  56.96573679,  29.35327205,  15.07479586,\n        49.39795207,  37.89516878,  55.27399095,  70.5287793 ,\n        34.96631774,  54.9593308 ,  55.06654355,  54.91532766,\n        77.45651561, 101.26344763,  40.35671192,  47.72859174,\n        84.29509919,  82.24451983,  29.43647485,  49.62186862,\n        44.52559469,  19.67293782,  28.19906789,  53.3988459 ,\n        76.98822075,  70.76100192,  78.58102673,  42.48809645,\n        49.50163486,  60.34105469,  97.2933164 , -11.63648322,\n        37.99545145,  22.63833621,  32.4570644 ,  54.51626858,\n        12.8345893 ,  52.88724172,  34.79923337,  53.49351012,\n        67.21403336,  44.4836404 ,  63.52270485,  61.64884962,\n        58.22986623,  44.88083416,  13.91709381,  48.76284166,\n        40.26437942,  86.22442121,  57.68535584,  68.62851517,\n        71.3348765 ,  36.94344385,  63.38509275,  53.05445266,\n        28.86990277,  44.70024564,  75.33577266,  57.81937316,\n        26.07899087,  42.28069837,  45.52787923,  55.90744697,\n        -3.9831072 ,  83.63294764,  59.18088379,  67.89096616])\n</code></pre> <pre><code>new_input_vals.mean()\n</code></pre> <pre><code>50.00000000000001\n</code></pre> <pre><code>new_input_vals.std()\n</code></pre> <pre><code>20.0\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"blog/2024/05/31/why-go-for-rag/","title":"Why go for RAG?","text":""},{"location":"blog/2024/05/31/why-go-for-rag/#overview","title":"Overview","text":"<p>In this project we choose a foundational model i.e. GPT or BERT and create an API that makes it easy to interact with the LLM.</p>"},{"location":"blog/2024/05/31/why-go-for-rag/#foundational-model","title":"Foundational model","text":"<p>We want a foundational model that can interact in the medical context. Some of the models considered here are :</p> <ul> <li>Medical Llama-8b - Optimized to address health related inquiries and trained on comprehensive medical chatbot dataset (Apache License 2.0) foundational model used here Meta-Llama-3-8b</li> <li>Llama3-OpenBioLLM-8B - fine tuned on corpus of high quality of biomedical data, 8 billion parameters. Incorporated the DPO data set</li> </ul>"},{"location":"blog/2024/05/31/why-go-for-rag/#approaches","title":"Approaches","text":"<p>To create a chat bot we have 2 approaches:</p> <ul> <li>Fine tuning existing foundational models on medical data set</li> <li>Create a Retrieval augmented generation framework which is used for retrieving facts from an external knowledge</li> </ul> <p></p>"},{"location":"blog/2024/05/31/why-go-for-rag/#fine-tuning-existing-foundation-models-on-medical-data-set","title":"Fine tuning existing foundation models on medical data set","text":"<ul> <li>Incorporates the additional knowledge into the model itself</li> <li>Offers a precise, succinct output that is attuned to brevity.</li> <li>High initial cost</li> <li>Minimum input size </li> </ul>"},{"location":"blog/2024/05/31/why-go-for-rag/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<ul> <li>Augments the prompt with external data</li> <li>Provides an additional context during question answering.</li> <li>Possible collision among similar snippets during the retrieval process</li> <li>RAG has larger input size due to inclusion of context information ,output information tends to be more verbose and harder to steer.</li> </ul>"},{"location":"blog/2024/05/31/why-go-for-rag/#experiment-conclusion","title":"Experiment Conclusion","text":"<p>GPT learned 47% of new knowledge with fine-tuning with RAG this number goes upto 72% and 74%.</p>"},{"location":"blog/2024/05/31/why-go-for-rag/#preferred-approach","title":"Preferred approach","text":""},{"location":"blog/2024/05/31/why-go-for-rag/#what-we-want","title":"What we want?","text":"<ul> <li>Fast Deployment option</li> </ul>"},{"location":"blog/2024/05/31/why-go-for-rag/#choice-of-approach","title":"Choice of Approach","text":"<p>RAG allows to create embeddings easily and allows for a fast deployment option.</p>"},{"location":"blog/2024/05/31/why-go-for-rag/#architecture","title":"Architecture","text":""},{"location":"blog/2024/05/31/why-go-for-rag/#references","title":"References","text":"<ul> <li>https://arxiv.org/pdf/2401.08406</li> </ul>"},{"location":"projects/arrhythmia/","title":"Arrhythmia Detection","text":""},{"location":"projects/arrhythmia/#project-link","title":"Project Link","text":""},{"location":"projects/arrhythmia/#introduction","title":"Introduction","text":"<p>We will be using the MIT data to train a model to detect Arrhythmias.</p>"},{"location":"projects/arrhythmia/#summary-of-project","title":"Summary of Project","text":"<p>We are using the ECG data collected by BIH and MIT, to see the distribution of the data you can check the </p>"},{"location":"projects/arrhythmia/#summary-of-the-data-after-analyzing","title":"Summary of the data after analyzing.","text":"<ul> <li>No. of records: 48</li> <li>Frequency : 360 samples per second</li> <li>Distribution: 25 male subjects between the ages of 32 and 89, 22 female subjects aged from 23 to 89 years. 60% of the total subjects were inpatient.</li> </ul>"},{"location":"projects/arrhythmia/#aim","title":"Aim","text":"<p>We will focus on classification of 5 classes, namely:</p> <ol> <li>Normal (N)</li> <li>Paced Beat (/)</li> <li>Right Bundle Branch Block Beat (R).</li> <li>Left Bundle Branch Block (L).</li> <li>Premature Ventricular Beat (V)</li> </ol>"},{"location":"projects/arrhythmia/#type-of-problem","title":"Type of problem","text":"<ol> <li>Classification problem</li> <li>Supervised Learning problem.</li> </ol>"},{"location":"projects/arrhythmia/#tools-and-frameworks","title":"Tools And Frameworks","text":"<ul> <li>WFDB</li> <li>Pytorch</li> <li>Pandas</li> <li>Seaborn</li> <li>py-ecg-detector</li> <li>imbalanced-learn</li> </ul>"},{"location":"projects/arrhythmia/#steps-to-implement-in-code","title":"Steps to Implement in Code","text":"<ol> <li>Break each record into fragments by detecting the peaks and taking a window before and after the peak</li> <li>Tranform the peak into feature vectors such that the number of dimensions is less than the original</li> <li>Fit the data to a model</li> <li>Report metrics</li> </ol>"},{"location":"projects/arrhythmia/#approaches","title":"Approaches","text":""},{"location":"projects/arrhythmia/#dimensionality-reduction-techniques","title":"Dimensionality Reduction Techniques","text":"<p>There are many dimensionality reduction methods, but some of the most common include:</p> <ul> <li>Principal component analysis (PCA) is a linear dimensionality reduction method that projects data points onto a lower-dimensional subspace in such a way that the variance of the data in the new space is maximized. This means that PCA finds the directions in the data that have the most variation, and projects the data points onto those directions.</li> <li>Factor analysis (FA) is another linear dimensionality reduction method that is similar to PCA. However, FA is designed to find latent factors that explain the variation in the data. These factors are not necessarily orthogonal, as they can be correlated with each other.</li> <li>Linear discriminant analysis (LDA) is a linear dimensionality reduction method that is specifically designed for classification tasks. LDA projects data points onto a lower-dimensional subspace in such a way that the classes are as well-separated as possible.</li> <li>Non-negative matrix factorization (NMF) is a nonlinear dimensionality reduction method that is often used for text analysis. NMF decomposes a matrix into two matrices, where the rows of the first matrix represent the original data points and the columns of the second matrix represent the latent factors.</li> <li>Sparse coding is a nonlinear dimensionality reduction method that is often used for image analysis. Sparse coding decomposes an image into a set of basis images, where each basis image is weighted by a coefficient. The coefficients are typically sparse, meaning that most of them are zero.</li> </ul> <p>These are just a few of the many dimensionality reduction methods that are available. The best method to use for a particular task will depend on the nature of the data and the goals of the analysis.</p> <p>Here are some of the benefits of using dimensionality reduction methods:</p> <ul> <li>Reduced computational complexity: When you reduce the dimensionality of your data, you also reduce the computational complexity of tasks such as clustering, classification, and regression. This can be a major advantage when you are working with large datasets.</li> <li>Improved visualization: When you reduce the dimensionality of your data, you can often visualize it more easily. This can be helpful for understanding the relationships between different variables and for identifying patterns in the data.</li> <li>Improved performance: In some cases, dimensionality reduction can actually improve the performance of machine learning models. This is because dimensionality reduction can help to remove noise from the data and to make the data more regular.</li> </ul> <p>However, there are also some potential drawbacks to using dimensionality reduction methods:</p> <ul> <li>Loss of information: When you reduce the dimensionality of your data, you lose some of the information in the original data. This can be a problem if the information that is lost is important for the task that you are trying to perform.</li> <li>Data distortion: In some cases, dimensionality reduction can distort the data. This can make it more difficult to interpret the results of your analysis.</li> <li>Overfitting: If you reduce the dimensionality of your data too much, you can end up overfitting your model to the training data. This can lead to poor performance on the test data.</li> </ul> <p>Overall, dimensionality reduction can be a powerful tool for data analysis. However, it is important to be aware of the potential drawbacks of these methods before using them.</p>"},{"location":"projects/arrhythmia/#loss-function-techniques","title":"Loss Function Techniques","text":"<p>Here are some of the most common loss functions used in machine learning:</p> <ul> <li>Mean squared error (MSE): This is the most common loss function for regression problems. It measures the squared difference between the predicted values and the actual values.</li> <li>Mean absolute error (MAE): This is another common loss function for regression problems. It measures the absolute difference between the predicted values and the actual values.</li> <li>Cross-entropy loss: This is the most common loss function for classification problems. It measures the difference between the predicted probabilities and the actual labels.</li> <li>Hinge loss: This is a loss function that is often used for support vector machines. It measures the distance between the decision boundary and the data points.</li> <li>Huber loss: This is a loss function that is less sensitive to outliers than MSE. It is often used for regression problems where there may be outliers in the data.</li> <li>Logistic loss: This is a loss function that is often used for logistic regression. It measures the log-likelihood of the data given the model parameters.</li> </ul> <p>The best loss function to use for a particular problem will depend on the nature of the data and the goals of the model. For example, MSE is often a good choice for regression problems where the data is normally distributed. Cross-entropy loss is often a good choice for classification problems where the classes are well-separated.</p> <p>It is important to note that loss functions are not always used to train machine learning models. In some cases, they can be used to evaluate the performance of a model after it has been trained. For example, you might use MSE to evaluate the accuracy of a regression model.</p>"},{"location":"projects/arrhythmia/#hyperparameter-tuning-approaches","title":"Hyperparameter Tuning Approaches","text":"<ol> <li>Grid Search </li> <li>Random Search</li> <li>Bayesian Optimization</li> </ol>"},{"location":"projects/arrhythmia/#machine-learning-signal-pre-processing-techniques","title":"Machine learning signal pre-processing techniques","text":"<p>We will be using the following pre-processing techniques:</p> <ol> <li>Fast Fourier Transform</li> <li>Discrete Wavelet Transform</li> <li>Calculate Feature Vectors by Simpsons Rule. Deep learning will not require pre-processing since it is meant to learn features by itself, we will however experiment and see the results.</li> </ol>"},{"location":"projects/arrhythmia/#machine-learning-approaches","title":"Machine Learning Approaches","text":"<p>We will be surveying the following algorithms in the is project:</p> <ol> <li>Logistic Regression</li> <li>Naive Bayes</li> <li>K-Nearest Neighbors</li> <li>Decision Tree</li> <li>Support Vector Machines</li> <li>Random Forest</li> </ol>"},{"location":"projects/arrhythmia/#deep-learning-approaches","title":"Deep Learning Approaches","text":"<p>We will be leveragin Supervised learning tasks:</p> <p></p> <p>We will be using CNNs for images primarily</p>"},{"location":"projects/arrhythmia/#goals-and-milestones","title":"Goals and Milestones","text":"<ul> <li>[x] Download the data</li> <li>[x] Summarise understanding of the data like the distribution and probabilites and other statical information</li> <li>[x] Pre-process the data</li> <li>[x] Train the data to create a machine learning model</li> <li>[x] Test the machine learning model with unseen data</li> <li>[x] Train the data to creat a deep learning model</li> <li>[x] Test the deep learning model with unseen data</li> <li>[x] Containerise Model for use with streamlit for easy future testing.</li> </ul>"},{"location":"projects/arrhythmia/#questions","title":"Questions","text":"Question Answers What is suppose to be the buffer size of the signal that we take? 200 How many classes do we consider? Refer this Do we always extract the QRS complex? Yes What are we looking for in an ECG Data? Leverage AI to extract features Do we Normalize or standardise the data the data? Yes"},{"location":"projects/arrhythmia/#resources","title":"Resources","text":"<ul> <li>https://realpython.com/python-scipy-fft/</li> <li>https://swharden.com/blog/2020-09-23-signal-filtering-in-python/</li> <li>https://danielmuellerkomorowska.com/2020/06/02/smoothing-data-by-rolling-average-with-numpy/</li> <li>https://refactored.ai/microcourse/notebook?path=content%2F06-Classification_models_in_Machine_Learning%2F02-Multivariate_Logistic_Regression%2Fmulticlass_logistic-regression.ipynb</li> <li>https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/</li> <li>https://towardsdatascience.com/how-to-convert-jupyter-notebooks-into-pdf-5accaef3758</li> </ul>"},{"location":"projects/arrhythmia/#processed-data","title":"Processed Data","text":"<p>https://drive.google.com/file/d/1ANq9kiWBsPsQpmsfJ-v4fpOMn0lEKypv/view?usp=sharing</p>"},{"location":"projects/arrhythmia/#useful-commands","title":"Useful commands","text":""},{"location":"projects/arrhythmia/#to-convert-notebooks-to-pdf","title":"To convert notebooks to PDF","text":"<pre><code> jupyter nbconvert --to pdf Machine\\ Learning.ipynb\n</code></pre>"},{"location":"projects/chest_x_rays/","title":"Corona Virus Detection in Chest X-Rays","text":""},{"location":"projects/chest_x_rays/#overview","title":"Overview","text":"This project is an attempt to develop a system that can classify lung X-ray images with an  emphasis to detect X-ray images with COVID-19. In this project we will make use of image  processing techniques learned in class and implement a classification system that can sort out  the COVID-19 X-rays from the X-rays that are normal and have Pneumonia. This project will  include image collection, image processing and image classification with the help of Deep neural  networks namely CNN. As an aside we will also explore the performance of a machine learning  model especially KNN. This project should yield a robust classifier that can help detect COVID-19 in a given image"},{"location":"projects/chest_x_rays/#introduction","title":"Introduction","text":"The new decade of the 21st century (2020) started with the emergence of a novel  coronavirus known as SARS-CoV-2 that caused an epidemic of coronavirus disease (COVID-19)  in Wuhan, China. It is the third highly pathogenic and transmissible coronavirus after severe acute  respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome  coronavirus (MERS-CoV) emerged in humans. [6] The virus, primarily affecting the respiratory  system, prompted a dire need for rapid and accurate diagnostic tools. Owing to the development of molecular biology technologies, molecular diagnostic  methods have developed rapidly. Among these, polymerase chain reaction (PCR)-based assays  are regarded as the gold standard for virus detection because of their high sensitivity and  specificity.[7] However, the test is too difficult to be widely used and it requires expensive  laboratory equipment and highly-trained laboratory staff. Additionally, it faces limitations in  sensitivity, impacting its effectiveness in certain scenarios.[8] X-ray imaging emerged as a crucial diagnostic method due to its ability to reveal distinct  lung patterns associated with COVID-19. Leveraging this characteristic, our project aims to  employ advanced deep learning and machine learning models to detect COVID-19 from X-ray  images. The researchers are trying to inculcate artificial intelligence (Machine learning or deep  2 learning models) for the efficient detection of COVID-19.[9] By amalgamating diverse datasets  encompassing COVID-19 X-ray images alongside normal and pneumonia-afflicted chest X-rays,  we seek to develop an automated classification system. This study compares the efficacy of deep learning, particularly Convolutional Neural  Networks (CNNs), with machine learning methodologies in identifying COVID-19 patterns within  X-ray images. The overarching goal is to contribute to the development of an intelligent diagnostic  tool that aids healthcare professionals in swift and accurate COVID-19 diagnoses based on X-ray  imagery."},{"location":"projects/chest_x_rays/#background-information","title":"Background Information","text":""},{"location":"projects/chest_x_rays/#what-is-covid-19","title":"What is COVID-19?","text":"This is a virus which has been circulating the globe since 2019 . This virus is accompanied  by symptoms like dry cough, fever, shortness of breath and a loss of smell and taste.This was  considered as a respiratory virus. It is reported that most cases of this disease are mild and after  running the full course of the virus most patients recover. However, the severe course of this  disease is pneumonia-like and may result in death. In some cases also we see symptoms like  headaches, aching limbs, sore throat and sniffles. As a consequence of this disease, patients  have experienced damages to their nerves and cardiovascular system. [1,2]"},{"location":"projects/chest_x_rays/#use-of-x-rays-as-an-alternative-to-testing-kit","title":"Use of X Rays as an alternative to testing kit","text":"When COVID-19 when was rampant, there was an increase in COVID-19 patients which  strained the healthcare systems around the world. At the time when COVID-10 was at its peak,  there were limited kits for diagnosis but also limited hospital beds, personal protective equipment  and ventilators. Due to the sheer volume of the patients, it becomes increasingly important to  distinguish patients with severe acute respiratory illness(SARI) and COVID-19.Soon the world  found itself using X-rays and devising tools with the help of it to classify X-rays. X-rays are not  only cheap but are commonly found in various diagnostic settings. Also, they are well integrated  into the digital infrastructure. Lastly, portable X-rays also allow for the creation of makeshift  isolation wards reducing the need of PPE kits[2]."},{"location":"projects/chest_x_rays/#related-works","title":"Related Works","text":"This idea is not a novel idea and there are many models and works surrounding X-rays.  Most of these use machine learning and deep learning techniques coupled with image processing  techniques that will help classify X-Ray images. The need for such systems was spurred on by  the lack of healthcare professionals to interpret the results. However, such systems are to be  used for triaging purposes [2].  <p>  Some works are:  </p>  Coronet[4], which is a Deep Convolutional Neural Network model that automatically  detects COVID-19 infection from chest X-ray images. The model the authors proposed is based  on Xception architectures with its weights pre trained on ImageNet. Transfer learning is then done  on the COVID-19 data set and pneumonia datasets which are publicly available. This model has  achieved an overall accuracy of 89.6%. The model was experimented with 4 class classification  scenarios(COVID,Pneumonia bacterial,Pneumonia viral,normal) and 3 class  classification(COVID,Pneumonia,normal). The model achieved an overall accuracy of 89.6% and  95% respectively.  <p> </p>  \"Automated detection of COVID-19 cases using deep neural networks with X-ray  images''[5], makes use of deep neural networks. However, the model so developed here was  experimented for 2 class classification(COVID, No Findings) and 3 class classification(COVID,  No findings, Pneumonia). The paper has implemented a DarkNet model that uses a you only look  once real time object detection system and has achieved an accuracy of 98% for binary  classification and 87% for multiclass classification.  <p> </p>  COVID-NET[6], it's the first open source network design for chest X-ray Images. In  addition to this the study creates a database for COVID-19 against which we can benchmark our  models and saves the trouble for creating the dataset. Transfer Learning approach was  undertaken here where the COVID-NET model was first pre trained on Imagenet and then on the  COVIDx dataset. 3 classes was used for classification and a comparison was done against other  pretrained models like ResNET-50 and VGG-19 and COVID-NET was found to perform better  than these models. The number of parameters and mathematical operations used in COVID-NET  was less compared to ResNET-50 and VGG-19."},{"location":"projects/chest_x_rays/#implemented-method","title":"Implemented Method","text":"<p>In our project we will have the following tasks:</p>"},{"location":"projects/chest_x_rays/#data-collection","title":"Data Collection","text":"<p>For our project we will be collecting data from 2 data sources: 1. Covid 19 X-Ray DataSet [11] 2. Bacterial and Viral Pneumonia</p>"},{"location":"projects/chest_x_rays/#covid-19-x-ray-dataset","title":"Covid 19 X-Ray DataSet","text":"<p>This data has the following distribution illustrated in the following image:</p> <p></p> <p>From this data set we take images that are labeled as COVID and the X-ray image is in the  anteroposterior (AP) or in the anteroposterior supine position(AP Supine).</p> <p>The COVID images were stored in the following formats:</p> <p></p>"},{"location":"projects/chest_x_rays/#bacterial-and-viral-pneumonia","title":"Bacterial and Viral Pneumonia","text":"In this data set we have a collection of chest X-rays that are either annotated as  Pneumonia or Normal. Most of the images present in this dataset are in the AP position only.  Pneumonia has about 4273 occurrences in the dataset whereas normal chest X-Rays has about  1583 occurrences.  After combining all the data our dataset size is 5855 instances of Chest X-ray images and  our final data distribution is illustrated in Fig 3"},{"location":"projects/chest_x_rays/#data-preparation","title":"Data Preparation","text":"After collecting the data we need to ensure that images are copied to a new directory  where they have the following structure:  <p>20% of the dataset is reserved for testing and 20% of the training data is used for  validation. We perform the following transformations on the images: 1. GrayScale Conversion - All the images have the same number of channels, i.e. all the  images should either be in RGB or grayscale. For this project we are going to convert all  images to grayscale. 2. Resize - All the images have the same dimensions i.e. 224x224 3. Center Crop - Crops a given image at the center 4. Normalize - Normalize a given image with mean = 0.5 and standard deviation = 0.5</p> <p>After doing the aforementioned processes our images will look as illustrated in Fig 4.</p> <p></p> <p>Optionally we also use Fourier Transform, to see whether we can improve the results.</p> <p></p>"},{"location":"projects/chest_x_rays/#building-our-model","title":"Building our model","text":""},{"location":"projects/chest_x_rays/#cnn","title":"CNN","text":"<p>In this project, rather than use a pretrained model, we built our own CNN Model. Our CNN  model architecture is as follows: 1. Input Convolution layer: This layer has a width of 224 and height of 224 and is composed  of a single channel and is processed through the convolution layer with a filter size of 7  and stride of 4. 16 filters are used here. Our output dimensions here are 55x55x16  2. 1st Hidden Convolution layer: The output dimensions in the previous layer serves as the  input here. We have 32 filters in this layer with a filter size of 5 and stride of 2. Our output  dimensions are then 26x26x32  3. 1st Max Pooling Layer: The output of the previous layer is then subject to max pooling  where the kernel size is 3 and stride is 2. The output dimensions here are 12x12x32  4. First Batch Normalisation layer: Batch Normalisation is performed here to speed up the  learning process. Output dimensions are the same as the input dimensions.  5. 2nd Hidden Convolution layer: The input size here is 12x12x32 which then undergoes  convolution through 64 filters with kernel size of 3 and stride of 1. The output dimension  here then would be 10x10x64  6. 3rd Hidden Convolution layer: The input size is 10x10x64 which then undergoes  convolution through 128 filters of kernel size 3 with a stride of 1. The output dimension  here would then be 8x8x128  7. 2nd Max Pool layer: The input size is 8x8x128 which undergoes max pooling against  kernel of size 3 with stride 1. The output dimension produced here is 6x6x128  8. 2nd Batch Normalisation layer: Batch Normalisation is performed here to speed up the  learning process. Output dimensions are the same as the input dimensions.  9. Flatten Layer: The output of the Batch Normalisation layer is then flattened so that the  dimensions are 1x4608  10. 1st Hidden Fully Connected Layer: The input size here is 1x4608 and the output size here  1x100.  11. 1st Dropout layer: The dropout percentage is 50%  12. 2nd Hidden Fully Connected Layer: The input size here is 1x100 and the output size here  is 1x50.  13. 2nd Dropout layer: The dropout percentage is 50%  14. Output layer: This is a fully connected layer whose input size is 1x50 and output is 1x3. The activation used throughout except in the output layer is ReLU which is given in Eq5. The output for each convolution is, </p> <p></p> <p>Where Ni is the number of data in a batch and Cout denotes the number of output channels, Cin  denotes the number of input channels. The output dimensions for the convolution layer are determined by the following formulas:  Where Hout and Wout are the output height and width and our final dimensions are  (Hout,Wout,Cout). In this case padding is 0 and dilation by default is 1 for all our convolution  layers.  The output for our MaxPool Layers is given in the following equation:</p> <p> Where h and w are our kernel sizes used in the max pooling layer. If the input dimensions to the max pool layer is (N,C,H,W) then our output dimensions will be  (N,C,Hout,Wout) where Hout and Wout calculations are given in Eq2. Our batch normalization layers follow the calculations given in Eq4.</p> <p> Where y is the output for a given x value and E[x] and Var[x] are the mean and variance and  sqrt(Var(x)) is the standard deviation. These are calculated for a given batch. Gamma and Beta  are learnable parameters. These vector sizes are the same as the channel size for the given  input.</p> <p></p> <p>The optimiser used for optimization is the Adam Optimizer which is given in Fig 7.</p> <p> The loss function used is Cross Entropy which is given Eq6. </p>"},{"location":"projects/chest_x_rays/#knn","title":"KNN","text":"<p>The KNN algorithm implemented in this project doesn't have a traditional architectural  design like a neural network. It follows a more straightforward approach based on distance  calculations and majority voting.The KNN algorithm implemented in this project follows these  steps:</p> <ol> <li>Retrieve Test Image - The code retrieves the test image at the current index from the  x_test tensor. This test image represents the data point that we want to classify. </li> <li>Calculate Distances to Training Set - The code calculates the Euclidean distance between  the test image and every image in the training set. The Euclidean distance is a measure  of similarity between two vectors. In this case, it represents the dissimilarity between the  test image and each training image. The distances are stored in the distance tensor.</li> </ol> <p>  3. Identify K Nearest Neighbors - The code identifies the k nearest neighbors of the test  image. This involves finding the k training images that have the smallest distances to the  test image. The torch.topk() function is used to select the k smallest distances and their  corresponding indices. The indices are stored in the indexes tensor.  4. Gather Class Labels of Nearest Neighbors - The code gathers the class labels of the k  nearest neighbors using the torch.gather() function. The class labels are stored in the  classes tensor.  5. Determine Majority Class - The code determines the majority class label among the k  nearest neighbors using the torch.mode() function. The majority class label is the class  label that occurs most frequently among the nearest neighbors. The majority class label  is stored in the mode variable.  6. Assign Predicted Class - The code assigns the majority class label to the test image as its  predicted class. This means that the test image is classified as the class that is most  represented among its k nearest neighbors. </p>"},{"location":"projects/chest_x_rays/#training","title":"Training","text":""},{"location":"projects/chest_x_rays/#cnn_1","title":"CNN","text":"<p>We run the training for the CNN model we have designed for about 30 epochs, but the  model doesn't necessarily train for 30 epochs since we add an early stopping condition i.e. training  is stopped as soon as validation loss exceeds the training loss. In addition to early stopping we  have dropout layers in place which prevent overfitting.</p> <p></p> <p></p>"},{"location":"projects/chest_x_rays/#knn_1","title":"KNN","text":"<p>The model defines a list of k_values, representing the possible values for the k  hyperparameter in the KNN algorithm. It then iterates through these values, training and  evaluating a KNN model for each k value.</p> <p>For each k value, the code calls the model to train and evaluate the KNN model. The  models take the training data , test data , and the k value as input. The code evaluates the  performance of each KNN model by comparing the predicted labels to the actual labels. It  calculates the number of correctly predicted labels and the accuracy. The accuracy represents  the percentage of test images that were correctly classified. The code tracks the highest accuracy achieved so far and the corresponding k value. This  allows it to identify the optimal k value for the given dataset.</p> <p></p>"},{"location":"projects/chest_x_rays/#evaluation","title":"Evaluation","text":""},{"location":"projects/chest_x_rays/#conclusion","title":"Conclusion","text":"<p>In this project we have done the following experiments: 1. CNN model training with Chest X-Ray images and also their fourier transform images. 2. KNN model training with Chest X-Ray images. There were no significant improvements in the results when the fourier transformed images used  for training in CNN. But with the fourier transformed images, the accuracy to classify COVID-19  is slightly higher, but overall there is not much improvement. Although CNN has a higher accuracy  than KNN, there are more computations involved, so it might be better to rely on KNN for resource  constraint applications of triaging COVID-19 patients. However KNN is not scalable so it's better  to rely on Deep CNN for improvements. To improve the model, structural optimization might need  to be carried out. Also, we could try the Transfer Learning approach, by training the network on  the Image Net data then on the Covid data. To appropriately benchmark our data, we should use  the COVIDx dataset, since the latest model COVID-NET has been trained on it and this is the  model to beat. Overall, our model doesn\u2019t perform as well as the reviewed works and more work  needs to be done to make the model better. Also, for KNN rather than training on raw data, we  need to see how well it performs on the feature vectors obtained from CNN.</p>"},{"location":"projects/chest_x_rays/#references","title":"References","text":"<p>[1]. ist COVID, W. What is COVID-19?.  [2]. Mangal, A., Kalia, S., Rajgopal, H., Rangarajan, K., Namboodiri, V., Banerjee, S., &amp; Arora, C.  (2020). CovidAID: COVID-19 detection using chest X-ray. arXiv preprint arXiv:2004.09803.  [3]. Narin, A., Kaya, C., &amp; Pamuk, Z. (2021). Automatic detection of coronavirus disease (covid19) using x-ray images and deep convolutional neural networks. Pattern Analysis and  Applications, 24, 1207-1220.  [4]. Khan, A. I., Shah, J. L., &amp; Bhat, M. M. (2020). CoroNet: A deep neural network for detection  and diagnosis of COVID-19 from chest x-ray images. Computer methods and programs in  biomedicine, 196, 105581.  [5]. Ozturk, T., Talo, M., Yildirim, E. A., Baloglu, U. B., Yildirim, O., &amp; Acharya, U. R. (2020).  Automated detection of COVID-19 cases using deep neural networks with X-ray images.  Computers in biology and medicine, 121, 103792.  [6]. Shen, M., Zhou, Y., Ye, J., Abdullah AL-maskri, A. A., Kang, Y., Zeng, S., &amp; Cai, S. (2020).  Recent advances and perspectives of nucleic acid detection for coronavirus. Journal of  Pharmaceutical Analysis, 10(2), 97\u2013101. https://doi.org/10.1016/j.jpha.2020.02.010   [7] Yashavantha Rao, H. C., &amp; Jayabaskaran, C. (2020). The emergence of a novel coronavirus  (sars\u2010cov\u20102) disease and their neuroinvasive propensity may affect in covid\u201019 patients. Journal  of Medical Virology, 92(7), 786\u2013790. https://doi.org/10.1002/jmv.25918   [8] Teymouri, M., Mollazadeh, S., Mortazavi, H., Naderi Ghale-noie, Z., Keyvani, V., Aghababaei,  F., Hamblin, M. R., Abbaszadeh-Goudarzi, G., Pourghadamyari, H., Hashemian, S. M., &amp; Mirzaei,  H. (2021). Recent advances and challenges of RT-PCR tests for the diagnosis of COVID-19.  Pathology - Research and Practice, 221, 153443. https://doi.org/10.1016/j.prp.2021.153443   [9] Das, S., Ayus, I., &amp; Gupta, D. (2023). A comprehensive review of covid-19 detection with  machine learning and Deep Learning Techniques. Health and Technology, 13(4), 679\u2013692.  https://doi.org/10.1007/s12553-023-00757-z   [10]. Wang, L., Lin, Z. Q., &amp; Wong, A. (2020). Covid-net: A tailored deep convolutional neural  network design for detection of covid-19 cases from chest x-ray images. Scientific reports, 10(1),  19549.  [11]. Cohen, J. P., Morrison, P., &amp; Dao, L. (2020). COVID-19 image data collection. arXiv preprint  arXiv:2003.11597.  [12]. https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html  [13]. https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html  [14]. https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html  [15]. https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html  [16]. https://pytorch.org/docs/stable/generated/torch.optim.Adam.html  [17]. https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html </p>"},{"location":"projects/tpu/","title":"Tensor Processing Unit","text":""},{"location":"projects/tpu/#overview","title":"Overview","text":"The purpose of this project is to implement the Multiply and Accumulate(MAC) Unit that can be used in the Tensor Processor Unit that optimizes matrix multiplication by integrating the computation unit as close to the memory as possible and reducing the read and write times to memory. The multiply and accumulate unit includes a multiplier and adder. In this project we have chosen to go with the Carry Look Ahead Adder for the adder and for the multiplier we implement a design with scan flip flops. In our design we use static CMOS logic to build all out gates i.e AND, OR, half adders. The result of our project is a schematic and physical design of the MAC unit that can operate at a frequency of 0.167GHz."},{"location":"projects/tpu/#introduction","title":"Introduction","text":"Deep learning models are like a \u201dSwiss Army Knife\u201d which are revolutionizing various fields. One example that comes to mind where is Healthcare. With the break through in image recognition in AI, we can create models that can help in chest X-ray or MRI scan diagnosis. Also, we have various machine learning models that play a vital role today in software engineering and research. Some problems that AI helps us solve today are image recognition, natural language processing and recommendation systems. In addition to this AI has a new frontier, generative AI [1]. Generative Adversarial Networks(GANs) are known to have high computational and memory requirements. The operations performed by GANs are convolution and deconvolution. These operations are not as compatible with conventional accelerators that are designed for convolution operations. There is a need for for customized accelerators or Application Specific Integrated Circuits like TPUs [2]. For the sake of this project we will consider neural network equations as shown in equation (1),(2),(3) and (4) where W are the weights, X are the inputs, b is the bias and f is the activation function. At the end of the day GANs are made up of two networks i.e. the generator and discriminator.   While there are several different hardware architectures for DNN acceleration, systolic array based implementations areshown to be most promising.The advantage with using systolic array based implementations that there need for buffering the inputs and routing is less complex. Theoretically, this should be energy efficient because we reducing the frequent reads for the weights and inputs. A general architecture for systolic based hardware architectures is shown in Fig. 1. MAC unit implementation is shown in Fig. 2 which is what we try to design in this project"},{"location":"projects/tpu/#literature-review","title":"Literature Review","text":"[4] Kuan-Chieh Hsu et al. proposes a General Purpose Computing architecture built on Edge Tensor Processing Units.This is an open source framework which allows researchers to easily use Neural Network accelerators for various applications. It was found that the proposed architecture is 2.46 times faster than CPU and the energy consumption is reduced by 40%. The aforementioned Edge Tensor Processing used is a trimmed down version of Google Cloud TPU i.e it has smaller data memory .   [5] Adam G. M. Lewis et al. in their paper have shown how to repurpose for large-scale scientific computation. They speed up matrix multiply calculations for QR decomposition and linear systems by distributing these in the matrix multiplication units in Google\u2019s Tensor Processing Units.   [6] Pramesh Pandey et al. proposes a low power near threshold TPU design without affecting the inference accuracies. The way they achieve this is by identifying error-causing activation sequences in the systolic array and preventing timing errors from the same sequence by booting the operating voltage for specific multiply and accumulate (MAC) units. The paper improves the performance of a TPU by 2-3 times without compromising the inference accuracies.   [7] Pramesh Pandey et al. proposes a way to solve the problem of underutilization of TPU systolic arrays. In their work they create of profile for idleness of the MAC units for different batch sizes. Also, they come up with an approach \u201cUPTPU\u201d, a low overhead power gating solution that can adapt to various batch sizes and zero weight computations   [8] Norman P. Jouppi et al. evaluate Google\u2019s Tensor Processing Unit (TPU), a custom ASIC accelerator for neural network inference deployed in their data centers since 2015. At the heart of the TPU is a 65,536 8-bit multiply-accumulate (MAC) matrix unit offering 92 TeraOps/s peak throughput and a large 28MB software-managed on-chip memory. The TPU\u2019s deterministic execution model better matches the 99th percentile response time requirements compared to the varying optimizations of CPUs/GPUs aimed at boosting average throughput. The TPU\u2019s relatively small size and low power are attributed to the lack of such complex features. Benchmarking using production neural nets representing 95% of datacenter inference demand, the TPU demonstrated 15X-30X higher performance and 30X-80X better TOPS/Watt compared to contemporary Haswell CPUs and K80 GPUs. Using the GPU\u2019s GDDR5 memory could potentially triple the TPU\u2019s TOPS and boost TOPS/Watt to 70X the GPU and 200X the CPU.   [9] Yang Ni et al. perform comprehensive characterization of the performance and power consumption of Google\u2019s Edge TPU accelerator for deep learning inference. They generate over 10,000 neural network models and measure their execution time and power on the Edge TPU. Key findings reveal non-linear relationships between metrics like the number of MACs and performance/power. Critical factors like onchip/off-chip memory usage are identified as having significant impact. Based on this characterization, the authors propose PETET, a machine learning-based modeling framework that can accurately predict Edge TPU performance and power consumption online with less than 10% error for new models.   [10] Kiran Seshadri et.al provide an in-depth analysis of the Fig. 3. Binary multiplication for 8 bits microarchitecture and performance characteristics of Google\u2019s Edge TPU accelerators for low-power edge devices. The authors first discuss the key microarchitectural details of three different classes of Edge TPUs spanning different computing ecosystems. They then present an extensive evaluation across 423K unique convolutional neural network (CNN) models to comprehensively study how these accelerators perform with varying CNN structures."},{"location":"projects/tpu/#functional-requirements","title":"Functional Requirements","text":"<p>The proposed MAC unit should meet the following requirements:</p>  \u2022 Implement high-performance multiplication and addition circuits capable of performing parallel multiplyaccumulate operations.   \u2022 Support configurable precision data formats to accommodate different neural network models and applications.   \u2022 Ensure low latenct and high throughput for the core matrix multiplication operations   \u2022 Implement strategies for efficient accumulation and storage of partial results.   \u2022 The operating frequency of atleast 1.2GHz.   \u2022 The MAC unit should satisfy equation (5).   \u2022 The inputs to the weights and inputs that the MAC unit accepts is 8 bits each.   \u2022 The final output is 24 bits.   \u2022 The multiplier should produce a 16 bit output as shown in Fig. 3.  <p></p>"},{"location":"projects/tpu/#design","title":"Design","text":"<p>In this section we discuss the design we wish to implement in our project.</p>"},{"location":"projects/tpu/#multiplier","title":"Multiplier","text":"In the design of the multiplier as shown in Fig. 4we make use of scan flip flops that allows us to load the values and shift them. Load bit stays high for one bit to allow us to load the values in the scan registers and in the next clock cycle the load bit is low which allows us to shift the values. Fig. 5 shows how a scan flip flop is designed using multiplexer and D flip-flops."},{"location":"projects/tpu/#adder","title":"Adder","text":"We use a carry look ahead adder(CLA) in the MAC unit as shown in Fig. 7. Table I has the truth table for the carry lookahead adder. Using 3 8 bit CLA Adders we create a 24 bit adder as show in Fig."},{"location":"projects/tpu/#design-alternatives","title":"Design Alternatives","text":"<p>An alternative to the MAC unit is discussed in this section.</p>"},{"location":"projects/tpu/#multiplier_1","title":"Multiplier","text":"<p>The hardware needed here if N was the number of bits we would need 8x8 hardware as shown in Fig. 8 and is much faster.</p> <p></p> <p></p> <p></p>"},{"location":"projects/tpu/#adder_1","title":"Adder","text":"<p>An alternative adder would be the Carry Select Adder which is one of the fastest adders as shown in Fig. 9</p>"},{"location":"projects/tpu/#design-calculations","title":"Design Calculations","text":""},{"location":"projects/tpu/#determining-nmospmos-ratio","title":"Determining NMOS/PMOS ratio","text":"<p>This ratio helps us size opir pmos given a nmos width. It is common for us to make use of equation 10. However, in reality we consider the ratio to be \u221a 2 To find this ratio we find the delays of 1-0 and 0-1 transitions and the rise and fall times. Ideally we want all these times to be equal, but its not possible.</p>"},{"location":"projects/tpu/#determining-fastest-clock-period","title":"Determining Fastest Clock Period","text":"<p>To determine the fastest clock cycle we need to run our simulations is Fast-Fast process variation we use the equations (11), (12) and (13). But, for our implementation we will lean towards equation (13).</p>"},{"location":"projects/tpu/#determining-power-consumption","title":"Determining Power Consumption","text":"<p>For practical purposes we will calculate the power consumption we will use equation (14).</p> <p></p> <p></p> <p></p>"},{"location":"projects/tpu/#floor-plan-and-area-calculations","title":"Floor Plan and Area Calculations","text":"<p>In this section before implementing the design we draw out the floor plan for each of circuits. Fig. 10, 11 and 12 show the floor plan design and area calculated.</p>"},{"location":"projects/tpu/#input-output-signals-and-power","title":"Input Output Signals and Power","text":"<p>From Fig. 11 the input signals are I0\u2212I7(inputs), W0\u2212W7(weights), A0\u2212A23(partial products)</p> <p>Output Signals from Fig. 11 are O0 \u2212 O2</p> <p>Power signal from the same diagram is VDD</p> <p>GND - VSS</p>"},{"location":"projects/tpu/#schematic-design","title":"SCHEMATIC DESIGN","text":"<p>In all of the schematic design we have used static CMOS logic. In Table II the sizes and timing information of the gates used in building the multipliers and adders has been summarized.</p> <p></p>"},{"location":"projects/tpu/#physical-layout-design","title":"Physical Layout Design","text":"The physical layout was made using the sticks diagram such that we tried to use merged contacts as much as possible. The design approach for the layouts was as follows:   \u2022 First a graph representation of the schematics for all our circuits was created.   \u2022 We tried to create a Eulerian path such the number of diffusion regions was reduced.   \u2022 Create short wires but using higher level of metal. In our implementation upto Metal 3 was used.   \u2022 The VDD and VSS signals were created with Metal 1 layers"},{"location":"projects/tpu/#summary-of-data-flow","title":"SUMMARY OF DATA FLOW","text":"<p>Two 8bit numbers are loaded into the multiplier in the MAC unit, The values are loaded by driving the LOAD bit high for atleast one clock cycle and remains low for the rest. Also, the partial products are loaded into the adder. The multiplier performs shifting and ANDs the outputs of the scan registers and sends the output to the adder.</p>"},{"location":"projects/tpu/#results","title":"Results","text":"<p>We made a full custom MAC unit whose area is 250 x 173 \u00b5m and can operate at frequency of 0.167 GHz</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"projects/tpu/#future-scope","title":"Future Scope","text":"<p>For the future scope of this project re-design the MAC unit with a smaller pmos and nmos ratio. Also, we could redesign our circuits with adiabatic logic.</p>"},{"location":"projects/tpu/#conclusion","title":"Conclusion","text":"<p>Circuit sizes are much larger thatn the 45nm standard cell library. We should use an nmos size that is \u2264 1um, since we did not do that our circuits are much larger. The MAC unit so designed in this project is not suitalbe for scalar multiplication since we need speeds \u2265 1GHz. Scan Flip Flop Multiplier uses less hardware but we need to synchronize when we load and shift patterns, so clocking is more complicated.</p>"},{"location":"projects/tpu/#references","title":"References","text":"<p>[1] R. Ferenc, T. Viszkok, T. Aladics, J. J \u00b4 asz, and P. Heged \u00b4 us, \u201cDeep- \u02dd water framework: The Swiss army knife of humans working with machine learning models,\u201d SoftwareX, vol. 12, p. 100551, Jul. 2020, doi: 10.1016/j.softx.2020.100551. [2] N. Shrivastava, M. A. Hanif, S. Mittal, S. R. Sarangi, and M. Shafique, \u201cA survey of hardware architectures for generative adversarial networks,\u201d Journal of Systems Architecture, vol. 118, p. 102227, Sep. 2021, doi: 10.1016/j.sysarc.2021.102227. [3] J. Zhang, K. Rangineni, Z. Ghodsi, and S. Garg, \u201cThundervolt,\u201d Research Gate, Jun. 2018, doi: 10.1145/3195970.3196129. [4] K.-C. Hsu and H.-W. Tseng, \u201cAccelerating applications using edge tensor processing units,\u201d SC \u201921: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1\u201314, Nov. 2021, doi: 10.1145/3458817.3476177. [5] A. G. M. Lewis, J. Beall, M. Ganahl, M. Hauru, S. B. Mallick, and G. Vidal, \u201cLarge-scale distributed linear algebra with tensor processing units,\u201d Proceedings of the National Academy of Sciences of the United States of America, vol. 119, no. 33, Aug. 2022, doi: 10.1073/pnas.2122762119. [6] P. Pandey, P. Basu, K. Chakraborty, and S. Roy, \u201cGreenTPU,\u201d DAC \u201919: Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1\u20136, Jun. 2019, doi: 10.1145/3316781.3317835. [7] P. Pandey, N. D. Gundi, K. Chakraborty and S. Roy, \u201dUPTPU: Improving Energy Efficiency of a Tensor Processing Unit through Underutilization Based Power-Gating,\u201d 2021 58th ACM/IEEE Design Automation Conference (DAC), San Francisco, CA, USA, 2021, pp. 325-330, doi: 10.1109/DAC18074.2021.9586224. [8] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, et al., Google, Inc., Mountain View, CA USA 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of ISCA \u201917, Toronto, ON, Canada, June 24-28, 2017, 12 pages. https://doi.org/10.1145/3079856.308024. [9] Ni, Y., Kim Y., Rosing, T., Imani, M. (2022). Online performance and power Prediction for Edge TPU via comprehensive characterization. 2022 Design, Automation ; Test in Europe Conference . https://doi.org/10.23919/date54114.2022.9774764. [10] Yazdanbakhsh, A., Seshadri, K., Akin, B., Laudon, J., Narayanaswami, R. (2022). An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks. IEEE International Symposium on Workload Characterization (IISWC). https://doi.org/10.1109/iiswc55918.2022.00017.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/artificial-neural-networks/","title":"Artificial Neural Networks","text":""},{"location":"blog/category/artificial-intelligence/","title":"Artificial Intelligence","text":""},{"location":"blog/category/llms/","title":"LLMs","text":""},{"location":"blog/category/attention/","title":"Attention","text":""},{"location":"blog/category/transformers/","title":"Transformers","text":""},{"location":"blog/category/vlms/","title":"VLMs","text":""},{"location":"blog/category/llava/","title":"LLaVA","text":""},{"location":"blog/category/hugging-face/","title":"Hugging Face","text":""},{"location":"blog/category/computer-vision/","title":"Computer Vision","text":""},{"location":"blog/category/image-generation/","title":"Image Generation","text":""},{"location":"blog/category/stable-diffusion/","title":"Stable Diffusion","text":""},{"location":"blog/category/tensorflow/","title":"Tensorflow","text":""},{"location":"blog/category/pytorch/","title":"Pytorch","text":""},{"location":"blog/category/computational-graphs/","title":"Computational Graphs","text":""},{"location":"blog/category/partial-differentiation/","title":"Partial Differentiation","text":""},{"location":"blog/category/dag/","title":"DAG","text":""},{"location":"blog/category/statistics/","title":"Statistics","text":""},{"location":"blog/category/hypothesis-testing/","title":"Hypothesis Testing","text":""},{"location":"blog/category/interview-question/","title":"Interview Question","text":""},{"location":"blog/category/image-classification/","title":"Image Classification","text":""},{"location":"blog/category/vision-transformers/","title":"Vision Transformers","text":""},{"location":"blog/category/distribution-modelling/","title":"Distribution Modelling","text":""},{"location":"blog/category/interview-questions/","title":"Interview Questions","text":""},{"location":"blog/category/git/","title":"git","text":""},{"location":"blog/category/matplotlib/","title":"Matplotlib","text":""},{"location":"blog/category/data-wrangling/","title":"Data Wrangling","text":""},{"location":"blog/category/nlp/","title":"NLP","text":""},{"location":"blog/category/vectordb/","title":"VectorDB","text":""},{"location":"blog/page/2/","title":"Blog","text":""}]}